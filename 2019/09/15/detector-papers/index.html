<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="包含r-cnn系列，yolo系列。r-cnn系列主要为二阶检测器，包含r-cnn，fast r-cnn，faster r-cnn，以及两个分支，用于实例分割的mask r-cnn（这个就暂时不研究了）以及基于focal loss的1阶检测器retinanet。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习图像detector相关论文整理">
<meta property="og:url" content="https://accioy.github.io/2019/09/15/detector-papers/index.html">
<meta property="og:site_name" content="Accio_">
<meta property="og:description" content="包含r-cnn系列，yolo系列。r-cnn系列主要为二阶检测器，包含r-cnn，fast r-cnn，faster r-cnn，以及两个分支，用于实例分割的mask r-cnn（这个就暂时不研究了）以及基于focal loss的1阶检测器retinanet。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://accioy.github.io/2019/09/15/detector-papers/smoothl1.svg">
<meta property="og:image" content="https://accioy.github.io/2019/09/15/detector-papers/focalloss.svg">
<meta property="og:updated_time" content="2019-11-18T08:13:28.177Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习图像detector相关论文整理">
<meta name="twitter:description" content="包含r-cnn系列，yolo系列。r-cnn系列主要为二阶检测器，包含r-cnn，fast r-cnn，faster r-cnn，以及两个分支，用于实例分割的mask r-cnn（这个就暂时不研究了）以及基于focal loss的1阶检测器retinanet。">
<meta name="twitter:image" content="https://accioy.github.io/2019/09/15/detector-papers/smoothl1.svg">
  <link rel="canonical" href="https://accioy.github.io/2019/09/15/detector-papers/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>深度学习图像detector相关论文整理 | Accio_</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Accio_</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">或者alow,renaud,ramyc,rym</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-shuoshuo">
      
    

    <a href="/shuoshuo/" rel="section"><i class="menu-item-icon fa fa-fw fa-tasks"></i> <br>说说</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tech">
      
    

    <a href="/categories/tech/" rel="section"><i class="menu-item-icon fa fa-fw fa-codepen"></i> <br>搬砖</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-diary">
      
    

    <a href="/categories/Diary/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>树洞</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-guestbook">
      
    

    <a href="/guestbook/" rel="section"><i class="menu-item-icon fa fa-fw fa-pencil"></i> <br>留言板</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://accioy.github.io/2019/09/15/detector-papers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="点点点">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Accio_">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">深度学习图像detector相关论文整理

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-09-15 01:58:41" itemprop="dateCreated datePublished" datetime="2019-09-15T01:58:41+08:00">2019-09-15</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-18 16:13:28" itemprop="dateModified" datetime="2019-11-18T16:13:28+08:00">2019-11-18</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/tech/" itemprop="url" rel="index"><span itemprop="name">tech</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2019/09/15/detector-papers/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习图像detector相关论文整理" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/09/15/detector-papers/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019/09/15/detector-papers/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>包含r-cnn系列，yolo系列。</p><p>r-cnn系列主要为二阶检测器，包含r-cnn，fast r-cnn，faster r-cnn，以及两个分支，用于实例分割的mask r-cnn（这个就暂时不研究了）以及基于focal loss的1阶检测器retinanet。</p><a id="more"></a>

<p>yolo系列，包含yolov1，yolov2，yolov3。</p>
<h1 id="yolo系列"><a href="#yolo系列" class="headerlink" title="yolo系列"></a>yolo系列</h1><h2 id="yolov1"><a href="#yolov1" class="headerlink" title="yolov1"></a>yolov1</h2><h3 id="模型输出的概率建模"><a href="#模型输出的概率建模" class="headerlink" title="模型输出的概率建模"></a>模型输出的概率建模</h3><p>图片首先被分割为S*S的网格（grid cell）。如果一个bbox的中心落在一个网格里，则该网格负责检测该物体。（对于pascal数据集，S定为7）</p>
<p>每个网格预测B个bbox及其confidence score，confidence定义为Pr(Object)∗IOU。 若该网格内没有物体，score应当是0；否则score被希望等于IOU（即如果网格不包含目标中心，则Pr(Object)=0，否则=1）。这个score反应了置信度，此处置信度是指模型预测的box是否真的包含目标（即第一项）以及模型自己认为box的准确度（即第二项）。</p>
<p>每个bbox包含5个预测值，分别为x,y,w,h和score。(x,y)坐标是box中心相对于网格边界（？），(w,h)是相对于整幅图像。score代预测box与真实值的iou。（iou不是能通过xywh直接算出来吗？）</p>
<p>每个cell同时还预测C个类别概率，即</p>
<script type="math/tex; mode=display">
\begin{equation}
\operatorname{Pr}\left(\text { Class }_{i} | \text { Object }\right)
\end{equation}</script><p>根据条件概率公式，有：</p>
<script type="math/tex; mode=display">
\operatorname{Pr}\left(\text { Class }_{i} \text { |Object }\right) * \operatorname{Pr}(\text { Object }) * \text { IOU }^{truth}_{pred}=\operatorname{Pr}\left(\text { Class }_{i}\right) * \text { IOU }^{truth}_{pred}</script><p>在原文中，对于PASCAL VOC数据集，有20类目标（C=20）,采用S=7，B=2，最终预测为</p>
<script type="math/tex; mode=display">
S \times S \times(B * 5+C)</script><p>即，7*7个网格，每个网格预测B个bbox，每个bbox有5个预测值，分别为xywh和score，以及20个类别概率值。</p>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>基于用于图片分类的googleNet，有一定的调整。两个网络模型，正常（yolo）的和tiny的(fast yolo)，正常的有24个卷积层+2个全连接层，tiny的有9个卷积层，滤波器也更少。使用了dropout在第一个全连接层之后（0.5）。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>imagenet上预训练分类，训练前20个卷积层，后面连了平均池化和一个全连接层。预训练使用的是224*224的输入，正式训练扩大了2倍长宽。使用了几何数据扩充。</p>
<p>输出xywh被归一化至0-1。激活函数使用leaky relu。</p>
<p><strong>虽然每个网格预测两个bbox，但实际训练仅采用与真值IOU较高的那个bbox，即每个网格实际仅有一个参与训练的有效预测。</strong></p>
<p>损失：使用sum-squared error。（和方差，其实和均方差没太大区别，只是没有除样本总数n）</p>
<p>具体包含三部分：</p>
<ol>
<li>Score置信度：标签为Pr(Object)∗IOU，若有物体中心落入该网格内，则Pr=1，否则Pr=0.</li>
<li>xywh：xywh都被归一化，仅在物体中心落入该网格时才在损失函数中出现。</li>
<li>类别预测：对于一个网格，如果物体中心落入，则对应类别label为1，其余为0。同上，仅在物体中心落入该网格时才在损失函数中出现，即此项预测值为物体存在条件下的概率，Pr(class_i | Object)。</li>
</ol>
<p>解决问题：大量不含目标的网格，即正负样本严重不均衡的问题：加权，含目标的加权至5，不含的至0.5。</p>
<p>解决问题：小box的小偏移误差比大box偏移误差更重要：预测wh的平方根而非值本身。</p>
<p>综上损失函数为：</p>
<script type="math/tex; mode=display">
\lambda_{\text {cord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\text {obj }}\left[\left(x_{i}-\hat{x}_{i}\right)^{2}+\left(y_{i}-\hat{y}_{i}\right)^{2}\right]
\\+\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\text {obj }}\left[(\sqrt{w_{i}}-\sqrt{\hat{w}_{i}})^{2}+(\sqrt{h_{i}}-\sqrt{\hat{h}_{i}})^{2}\right]
\\+\sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\mathrm{obj}}\left(C_{i}-\hat{C}_{i}\right)^{2}
\\+\lambda_{\mathrm{noobj}} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\mathrm{noobj}}\left(C_{i}-\hat{C}_{i}\right)^{2}
\\+\sum_{i=0}^{S^{2}} \mathbb{1}_{i}^{\mathrm{obj}} \sum_{c \in \text { classes }}\left(p_{i}(c)-\hat{p}_{i}(c)\right)^{2}</script><p>注意，1^obj_i代表物体中心落在网格i内，1^obj_ij代表网格i的第j个box负责预测该物体。因此该损失函数仅惩罚含有物体中心的网格分类，仅惩罚负责预测那个物体的bbox坐标。</p>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>对大型物体，可能有多个cell同时给出较为准确的预测，可以理解，大型物体可能有好几个cell靠近中心点。使用nms筛选。</p>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p>空间局限性：可预测的数量有限（98个，并且有空间独立）。对小的集群目标性能较差。对异形目标（长宽比）较差。对大小尺寸目标损失函数有问题。</p>
<h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>yolov1出的时候，faster r-cnn已经出了，但好像还没开源（？）,而且这个时候精度还不如fast r-cnn，原文主要对比了fast r-cnn。此外原文还提到了Deformable parts models，Deep MultiBox，OverFeat，MultiGrasp等。</p>
<p>对比faster r-cnn，yolo在pascalvoc数据集上速度更快，但精度较低。</p>
<p>YOLO PASCAL2007+2012 mAP63.4 FPS45 输入448*448</p>
<h2 id="yolov2"><a href="#yolov2" class="headerlink" title="yolov2"></a>yolov2</h2><p>又叫yolo9000，号称可以实现识别9000类目标。基于yolo改进，超越基于resnet的faster r-cnn和ssd并且更快。可以预测没有标签的目标类别。</p>
<p>提出一种新的训练方法，可以实现同时利用检测数据集和分类数据集。</p>
<h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><p>引入BN</p>
<p>改进基于高分辨率的预训练方法</p>
<p>引入anchor boxes：优点：预测补偿（offsets）而非坐标本身，对网络来说更易于学习。移除原来的FC层，使用ancor boxes进行预测。</p>
<p>anchor dimension聚类：不同于R-CNN手动选取anchor box的数量和尺度，本文使用<strong>k-means聚类</strong>计算box dimensions的先验信息。根据训练集所有的boxes使用k-means聚类计算出的k个box尺度即定为anchor的尺度。综合性能和计算速度，取k等于5。</p>
<p>在k-means计算时，距离公式以IOU为标准。公式如下：</p>
<script type="math/tex; mode=display">
d(\text { box }, \text { centroid })=1-\operatorname{IOU}(\text { box, centroid })</script><p>注意：此时的IOU计算仅考虑形状，不考虑位置。</p>
<p>位置预测：为使模型更容易收敛，预测相对于cell偏移值，坐标预测值落于0-1范围内（使用sigmoid激活函数）。</p>
<p>模型对每个特征图像素点预测5个bbox，每个bbox预测5个值。</p>
<script type="math/tex; mode=display">
\begin{aligned} b_{x} &=\sigma\left(t_{x}\right)+c_{x} \\ b_{y} &=\sigma\left(t_{y}\right)+c_{y} \\ b_{w} &=p_{w} e^{t_{w}} \\ b_{h} &=p_{h} e^{t_{h}} \end{aligned}\\\operatorname{Pr}(\text { object }) * I O U(b, \text { object })=\sigma\left(t_{o}\right)</script><p>注意：此处预测仍然是相对于网格。对于416×416的输入，特征图为13×13，即每个特征图像素点对应原图32×32，即一个网格。由于每个cell对应到特征图上为一个像素点，因此坐标预测范围理应在0-1之间。</p>
<p>细粒度特征：yolo在13<em>13的特征图上预测。加了一条passthrough layer带来前一层26\</em>26的特征。通过一个神秘（？）的调整把 26×26×512 的特征图调整到了 13×13×2048 ，然后和原来的特征图进行concatenate。</p>
<p>多尺度训练：模型仅使用卷积和池化层，因此与输入尺度无关。训练模型使其适应多尺度输入。每10个batches随机换一个输入尺度： {320,352,…,608}. </p>
<p>性能：YOLOv2 416×416 2007+2012 mAP76.8 FPS67 </p>
<p>特征提取器：大多检测系统基于VGG16，但这玩意很笨重。本文提出了Darknet-19分类模型，作为Yolov2的基底。</p>
<p>输出：先进行224<em>224分辨率的1000类分类训练，然后在448分辨率fine tune。然后溢出最后一个卷积层，加入一个新的卷积层，有1024个滤波器，每个跟着一个1\</em>1卷积，层数由输出纬度决定。对VOC数据集，预测5个anchors，每个anchor对应4个坐标值、一个置信度、20个类别概率，因此输出为5×（1+4+20）=125层通道。</p>
<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>识别与检测混合训练法：混合二者数据，若图片带有检测框标签，则进行完整的loss计算与反向传播，若图片仅有类别信息，则只根据分类损失进行反向传播。</p>
<p>问题：使用softmax分类损失的数学前提是所有类别是exclusive的，而一般分类数据集标签会有细分之类的问题，并不是exclusive的。针对这个问题，作者建立了层级类别标签树（wordTree），对每一个层级（是互斥的）进行softmax分类。对于预测概率，使用条件概率公式，并且假定每个图片包含object的概率为1。</p>
<p>在检测时，对每个box都预测一个概率树（对应类别树），自顶向下取最大分支直至绝对概率达到某个门限，即预测这个类别。</p>
<p>测试结果表明，这种方法使得模型对仅有类别训练没有位置训练的类型有一定的检测泛化能力。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>看完论文后总觉得缺点什么，原来是没有损失函数，这部分原文里没有提到，函数式从网上抄的。总的来说，损失函数包含了上述内容大多关键点，还是很有必要分析一下的。</p>
<script type="math/tex; mode=display">
\begin{aligned} \operatorname{loss}_{t}=\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} 1_{\text {Max } IOU<Thresh} \lambda_{\text {noobj}} *\left(-b_{i j k}^{o}\right)^{2} \\+1_{t<12800} \lambda_{\text {prior}} * \sum_{r \epsilon(x, y, w, h)}\left(\text {prior}_{k}^{r}-b_{i j k}^{r}\right)^{2} \\+1_{k}^{\text {truth}}\left(\lambda_{\text {cord}} * \sum_{r \epsilon(x, y, w, h)}\left(t r u t h^{r}-b_{i j k}^{r}\right)^{2}\right.\\+\lambda_{o b j} *\left(\operatorname{IOU}_{\text {truth}}^{k}-b_{i j k}^{o}\right)^{2} \\\left.+\lambda_{\text {class}} *\left(\sum_{c=1}^{c}\left(\operatorname{truth}^{c}-b_{i j k}^{c}\right)^{2}\right)\right) \end{aligned}</script><p>W和H为最终特征图的分辨率（13×13），A为每个网格（特征图像素点）的anchor数目，原文取5。λ为各项权重。bo为置信度score，br为box的坐标和大小，bc为类别概率。</p>
<p>对特征图上所有anchors进行遍历：</p>
<p>第一项：若anchor与所有真值的最大IOU小于门限（原文门限取0.6），则计算该项，为no object的损失计算。此时给出的置信度越大则损失越大，即计算置信度与0的L2。</p>
<p>第二项：仅在训练初期（已训练样本数量小于12800时）计算。该项计算的是真值中心点不在对应网格内，但IOU大于门限的anchor，计算预测值与先验box的差距。即希望这种情况网络不进行任何预测。</p>
<p>第三项：真值Box中心落在对应网格内，仅匹配IOU最大的那个先验anchor进行计算，其余的anchors，除非IOU小于门限值被计入第一项，否则直接忽略（这一点类似于yolov1）。计算box与真值的损失，置信度损失（根据yolov1的条件概率公式，置信度与IOU进行L2计算），以及类别概率损失。</p>
<h2 id="yolov3"><a href="#yolov3" class="headerlink" title="yolov3"></a>yolov3</h2><h3 id="改进-1"><a href="#改进-1" class="headerlink" title="改进"></a>改进</h3><p>首先，IOU门限被改至0.5。</p>
<p>分类：使用多标签分类，移除softmax，因为作者发现这玩意对于提升性能并不是必要的。取而代之，使用independent logistic classiﬁers（逻辑回归），训练中使用二分类交叉熵损失。</p>
<p>多尺度特征：使用了3个不同尺度（类似于FPN）。输出仍然包括了bbox，置信度，类别概率。输出维度为N ×N ×[A∗(4+1+C)] ，N为特征图边长，A为每个点的box数量，原文对COCO数据集取3，4为box的4个坐标，1为置信度，C为类别概率向量。除了原框架的最后一层外，使用之前两层进行2倍上采样，并且与更之前的特征图进行融合，从而同时得到有意义的语义和细粒度信息。</p>
<p>Anchor尺度的先验聚类：仍然使用k-means聚类，只不过这次k取9。并且，根据结果的尺度，给三个尺度的特征图进行分配，每个特征图分配三个尺度的anchors。具体分配方式大致为，分辨率较高的特征图分配感受野较小的anchors，分辨率较低的特征图分配感受野较大的anchors，从结果上说这与retinanet基本一致。</p>
<p>更新主干网络：从darknet19更新至darknet53。</p>
<p>性能：从mAP上看，yolov3并没有比retinanet好，但有一说一，这玩意计算速度大约是retinanet的三倍。</p>
<p>特点：yolov3相比前两代在小目标检测上有大幅提升，但对于中型和大型目标有丶差。</p>
<p>作者有试过但没成功的idea：坐标预测，直接预测线性补偿；focal loss：作者认为可能是因为yolo本身有基于条件概率的置信度预测，因此不存在大量背景的负样本造成的不均衡问题；双门限：在faster r-cnn中，IOU大于0.7的被认为是正样本，小于0.3的被认为是负样本（背景），其余被忽略，而在yolo中，这样做并没有得到一个好结果。</p>
<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>原文这次同样没有提到损失函数的具体公式，但根据上述内容不难看出，Yolov3相对于yolov2在损失函数上的变化主要为在分类中使用了逻辑回归和交叉熵损失。根据源码，公式应该如下：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\text { loss (object) }=\lambda_{\text {coord }} \sum_{i=0}^{K \times K} \sum_{j=0}^{M} I_{ij}^{obj}\left[\left(x_{i}-\hat{x}_{i}\right)^{2}+\left(y_{i}-\hat{y}_{i}\right)^{2}\right]+} \\ {\quad \lambda_{\text {coord}} \sum_{i=0}^{K \times K} \sum_{j=0}^{M} I_{j}^{o b j}\left(2-w_{i} \times h_{i}\right)\left[\left(w_{i}-\hat{w}_{i}\right)^{2}+\left(h_{i}-\hat{h}_{i}\right)^{2}\right]-} \\ {\sum_{i=0}^{K \times K} \sum_{j=0}^{M} I_{i j}^{obj}\left[\hat{C}_{i} \log \left(C_{i}\right)+\left(1-\hat{C}_{i}\right) \log \left(1-C_{i}\right)\right]-} \\ {\quad \quad \lambda_{\text {noobj}} \sum_{i=0}^{K \times K} \sum_{j=0}^{M} I_{i j}^{\text {noobj}}\left[\hat{C}_{i} \log \left(C_{i}\right)+\left(1-\hat{C}_{i}\right) \log \left(1-C_{i}\right)\right]} \\ {\sum_{i=0}^{K \times K} I_{i j}^{o b j} \sum_{c \in \text { classes }}\left[\hat{p}_{i}(c) \log \left(p_{i}(c)\right)+\left(1-\hat{p}_{i}(c)\right) \log \left(1-p_{i}(c)\right)\right]}\end{array}</script><p>对于无obj的情况，仅计算置信度，不同于之前的L2，此处采用交叉熵作为置信度的损失函数。同理在有obj的情况，置信度和分类概率都采用交叉熵。对于有无obj的判断原理同之前，只是门限被改至0.5。同样，在每个网格仅有一个anchor即IOU最高的anchor被赋予真值，其余的不参与计算。</p>
<h1 id="R-CNN系列"><a href="#R-CNN系列" class="headerlink" title="R-CNN系列"></a>R-CNN系列</h1><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p>本文发表于2014年。</p>
<h3 id="背景及整体框架"><a href="#背景及整体框架" class="headerlink" title="背景及整体框架"></a>背景及整体框架</h3><p>背景：将CNN在图像分类领域的成功（2012年）应用于目标检测上面。检测问题：一种方式是使用滑窗检测器，即CNN。在当时，在整幅图像上做滑窗检测有技术难题，因此没有采用。而是使用了一种叫recognition using region的模型，在之前被证明有效。在测试阶段，会从图像提取约2000个候选框，并使用CNN进行特征提取，使用afﬁne image warping技术使得CNN的输入维度无视region形状固定，从而使得输出固定，然后使用支持向量机（SVM）进行分类。</p>
<p>另一个问题：在当时，有目标位置标签的数据集还不够多。作者使用无监督学习进行预训练，然后用有监督学习进行fine tune。使用了非极大值抑制筛选（NMS），使用了bounding-box regression算法，并分析证明这个算法在模型中很关键。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>综上，整个模型分为三个部分，第一部分生成候选区域，第二部分为CNN用于从每个候选区域提取一个固定维度的特征向量，第三部分是一个SVM用于分类。</p>
<h4 id="区域建议"><a href="#区域建议" class="headerlink" title="区域建议"></a>区域建议</h4><p>已有很多方法，本文采用的是一种叫selective search的方法。原文没有对该方法进行描述仅给出了相关参考文献。我觉得现在深度学习模型也不用这些东西了，所以就不对它们进行研究了。</p>
<h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><p>不论候选区域的形状大小，一律放缩变形至227×227，CNN有5个卷积层2个全连接层，最终输出为一个4096的向量。</p>
<h3 id="预测-1"><a href="#预测-1" class="headerlink" title="预测"></a>预测</h3><p>预测流程：对图片进行selective search，提取出大约2000个区域，每个变形后输入CNN，提取出特征向量，然后用SVM打分，每一个类别都训练了一个SVM。然后使用NMS筛选，对一堆互相之间IOU超过门限的候选框仅保留最大得分的框。</p>
<p><strong>特别注意：NMS筛选是基于每个类别独立筛选的！这样做的好处在于可以处理重叠（遮挡）的情况，但问题（我实际碰到的）是，如果数据集中有两种或多种较为相似的目标，而得分门限设置又不高的情况下，可能会出现多个IOU极高的检测结果。</strong></p>
<h3 id="训练-2"><a href="#训练-2" class="headerlink" title="训练"></a>训练</h3><p>CNN进行了分类预训练。然后用候选区域进行了分类训练。注意背景也被当作一个类别加入，在候选区域与真值IOU大于0.5的时候被当作正样本，否则为负样本。为了避免不均衡问题，正负样本比例被控制在1:3。</p>
<p>对SVM二分类器的训练，正负样本的定义有所不同，ground truth为正样本，对于负样本的定义，经过IOU上的搜索，门限被定为0.3，即0.3以下被当作负样本。对原数据集的每一个类别训练了一个二分类的SVM。</p>
<p>总的来说，R-CNN训练分为三步，首先基于selective search的建议区域训练CNN分类，然后基于CNN的特征训练SVM分类，然后基于CNN的特征训练bounding box regression。</p>
<h3 id="可视化分析与结构分析"><a href="#可视化分析与结构分析" class="headerlink" title="可视化分析与结构分析"></a>可视化分析与结构分析</h3><p>R-CNN提到了一些可视化分析方法的参考文献，以后有时间可以看一看。</p>
<p>本文里用的方法是，使用大量的建议区域作为网络输入，检查特定单元的激活程度，按照激活程度对建议区域进行排序，并使用NMS筛选。</p>
<p>结构分析发现，在没有fine tune的情况下，移除掉最后两层FC层并没有降低多少mAP，尽管这两层占用了大多数的参数量，CNN的power主要来自于其卷积层。在有fine tune的情况下，最后两层FC层带来的精度提升明显。</p>
<p>CNN结构选择对精度影响明显。</p>
<p>另外，BBox regression是在DPM里提出的，在本文也用于定位。对于一个selective search的区域，使用CNN的特征（移除最后两个FC）训练一个线性回归器，来预测一个新的检测窗。</p>
<p>原文还提到了语义分割方面的迁移应用，不做仔细研究。</p>
<h3 id="附：关于Bounding-box-regression"><a href="#附：关于Bounding-box-regression" class="headerlink" title="附：关于Bounding-box regression"></a>附：关于Bounding-box regression</h3><p>输入：两组box坐标（中心xy和长宽wh），一组为ground truth，记作G，另一组为（前一阶段的）建议值，记作P。该算法的目标是学习一个变形，可以将P变换至G。</p>
<p>对于神经网络来说，首先对输入经过特征提取，然后输出结果。这个过程可以抽象为一组函数，记作d(P)。在这里，原文令：</p>
<script type="math/tex; mode=display">
d_{\star}(P)=\mathbf{w}_{\star}^{\mathrm{T}} \boldsymbol{\phi}_{5}(P)</script><p>其中Φ(P)是神经网络最后一层输出，经过一个线性函数变换得到d(P)。此处这个线性函数我觉得主要是为了进行维度上的变换，将神经网络的输出压缩至一个4×1的向量，另一方面应该是神经网络在上文中主要承担分类方面的特征提取，因此对于位置修正，这里需要额外的参数来训练。对于这些参数的训练方法为普通的L2 loss加正则项：</p>
<script type="math/tex; mode=display">
\mathbf{w}_{\star}=\underset{\hat{\mathbf{w}}_{\star}}{\operatorname{argmin}} \sum_{i}^{N}\left(t_{\star}^{i}-\hat{\mathbf{w}}_{\star}^{\mathrm{T}} \boldsymbol{\phi}_{5}\left(P^{i}\right)\right)^{2}+\lambda\left\|\hat{\mathbf{w}}_{\star}\right\|^{2}</script><p>即令d(P)向t<em>（真实的变换）回归。t\</em>的定义为：</p>
<script type="math/tex; mode=display">
\begin{aligned} t_{x} &=\left(G_{x}-P_{x}\right) / P_{w} \\ t_{y} &=\left(G_{y}-P_{y}\right) / P_{h} \\ t_{w} &=\log \left(G_{w} / P_{w}\right) \\ t_{h} &=\log \left(G_{h} / P_{h}\right) \end{aligned}</script><p>这样定义的原因是：我们需要定义一个变形方式，这个变形方式进行“变换的程度“应当只与P和G的偏移量有关，而与box本身的尺寸形状无关。因此使用上述方式计算偏移的相对量。</p>
<p>根据上式，原始建议区域P经过上述模型的特征提取后输出d(P)，计算修正后的box的公式如下，即对上式进行逆变换：</p>
<script type="math/tex; mode=display">
\begin{aligned} \hat{G}_{x} &=P_{w} d_{x}(P)+P_{x} \\ \hat{G}_{y} &=P_{h} d_{y}(P)+P_{y} \\ \hat{G}_{w} &=P_{w} \exp \left(d_{w}(P)\right) \\ \hat{G}_{h} &=P_{h} \exp \left(d_{h}(P)\right) \end{aligned}</script><p>原文强调了两个注意点：一是优化函数中的正则项很重要，二是如果P和G相差太多，则该算法没有意义。为了保证第二点，事先计算P与G的IOU，大于门限（原文取0.6）才进行上述回归计算。</p>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p>本文发表于2015年。</p>
<p>R-CNN的缺点：计算慢，训练繁琐，由于多步训练特征要被写入硬盘，消耗资源多。它对每一个建议区域都要单独计算。</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>R-CNN从整幅原始图像提取特征。</p>
<p>对于每一个建议区域，使用ROI池化层从特征图中提取一个固定长的特征向量。</p>
<p>每一个特征向量被喂进一系列FC层中，然后传到两个输出层：一个输出softmax分类概率，注意背景也被当作一个类别在其中；另一个输出bounding box回归的4个参数。</p>
<p>ROI池化层：使用最大池化，将任意感兴趣区域的特征转化为一小块有着固定分辨率（H×W）的特征图。方法为将原区域分割为H×W的小块，每一小块进行最大池化，不同通道独立进行。</p>
<p>注：region of interest (RoI)感兴趣区域</p>
<p>综上，整个模型的输入为原始图像以及该图像的感兴趣区域列表，感兴趣区域仍然来着selective search，数量仍然大约为2000，输出为类别分类（包括背景）以及bbox回归值。</p>
<h3 id="多任务损失函数"><a href="#多任务损失函数" class="headerlink" title="多任务损失函数"></a>多任务损失函数</h3><script type="math/tex; mode=display">
L\left(p, u, t^{u}, v\right)=L_{\mathrm{cls}}(p, u)+\lambda[u \geq 1] L_{\mathrm{loc}}\left(t^{u}, v\right)</script><p>其中第一项为分类损失，采用普通的softmax交叉熵损失，公式如下：</p>
<script type="math/tex; mode=display">
L_{\mathrm{cls}}(p, u)=-\log p_{u}</script><p>其中p为预测概率，u为真实类别。再次注意此处分类包含背景类。</p>
<p>第二项为bounding box regression损失，其中v是bbox回归的目标值，t^u为预测值。[u≥1]代表进对与真值box匹配了的box做计算，即对于背景，u=0。λ为损失的权重，原文中取1。</p>
<p>对于bbox回归：</p>
<script type="math/tex; mode=display">
L_{\mathrm{loc}}\left(t^{u}, v\right)=\sum_{i \in\{x, y, w, h\}} \operatorname{smooth}_{L_{1}}\left(t_{i}^{u}-v_{i}\right)</script><p>其中：</p>
<script type="math/tex; mode=display">
\operatorname{smooth}_{L_{1}}(x)=\left\{\begin{array}{ll}{0.5 x^{2}} & {\text { if }|x|<1} \\ {|x|-0.5} & {\text { otherwise }}\end{array}\right.</script><p>相对于R-CNN中采用的L2 loss，作者认为该loss更加稳定。</p>
<p><img src="/2019/09/15/detector-papers/smoothl1.svg" alt="untitled"> </p>
<p>在本文中，采用IOU大于0.5的预选框进行回归计算。IOU在0.1到0.5之间的被当作背景。</p>
<h3 id="其它技巧与结论"><a href="#其它技巧与结论" class="headerlink" title="其它技巧与结论"></a>其它技巧与结论</h3><p>由于ROI池化层结构特殊，原文提到了其反向传播的计算，此处不做深入研究。</p>
<p>探索了两种方法使模型对尺度鲁棒，brute force和 image pyramids。</p>
<p>输出同样进行了NMS筛选，<strong>并且仍然是每个类别独立筛选</strong>。</p>
<p>可以使用truncated SVD对FC层的计算进行加速。</p>
<p>作者证明了多任务系统训练的有效性（因为共享特征表述）。</p>
<p>多尺度训练对精度提升几乎没有效果。softmax分类略优于SVM。</p>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p>本文发表于2016年。</p>
<p>提出了 Region Proposal Network (RPN) （区域建议网络）来替代之前的selective search。实现真正由神经网络完成的端到端的目标检测识别。Selective search运算已经成为之前检查系统的速度瓶颈。</p>
<p>提出anchor boxes的机制。</p>
<h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><p>区域建议网络是一个全卷积网络，因此可以接收任何尺寸的图片作为输入，输出若干建议区域及其score。</p>
<p>首先基于原图由CNN提取特征。然后在特征图上进行n×n的滑窗（原文取n=3），并输入到两个FC层中，一个用于 box-regression 另一个用于分类。两个FC层对全图参数共享，因此实际实现上，结构为在原特征图上进行n×n卷积，然后接入两个并行的1×1卷积。</p>
<h4 id="anchors"><a href="#anchors" class="headerlink" title="anchors"></a>anchors</h4><p>对于滑窗，每一个位置预测k个框。因此box regression输出维度为4k，分类输出维度为2k，注意此时的分类仅仅针对objectness，即仅区分含有目标或者是背景，不进行具体的识别分类。对于位置预测，要进行BBox回归，需要有先验box，此处在每个位置定了9个先验box，3种大小和3种长宽比。即k=9。若特征图大小为W×H，则共有W×H×k个先验框。这样的方式保证了平移不变性。</p>
<h4 id="RPN的损失函数"><a href="#RPN的损失函数" class="headerlink" title="RPN的损失函数"></a>RPN的损失函数</h4><p>由上述可知，RPN的损失包含两项，分别为box回归损失和二分类损失。对于anchor boxes样本的正负判定，条件为：选取与真值IOU大于0.7的box为正样本，或者选取IOU最高的box为正样本。第二条是为了保证正样本一直存在，通常第一条已经足够。注意，一个真值可能匹配到多个先验anchor boxes；与任何真值IOU小于0.3的anchor boxes被定为负样本（即背景）；其余的anchor boxes不参与训练。</p>
<p>该结构还有一个巧妙之处在于，对于不同尺寸和长宽比的先验anchor boxes，计算回归的权重是不共享的。</p>
<p>综上，损失函数如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} L\left(\left\{p_{i}\right\},\left\{t_{i}\right\}\right)=& \frac{1}{N_{c l s}} \sum_{i} L_{c l s}\left(p_{i}, p_{i}^{*}\right) \\+\lambda & \frac{1}{N_{r e g}} \sum_{i} p_{i}^{*} L_{r e g}\left(t_{i}, t_{i}^{*}\right) \end{aligned}</script><p>对于正样本，p<em>=1，对于负样本，p\</em>=0。分类loss采用交叉熵，回归算法仍然采用bounding box regression，loss和之前的模型一样采用smoothL1。权重λ原文设为10，并提到模型对该参数在很大范围内并不敏感。</p>
<p>训练时，控制正负样本比例在1:1。</p>
<h3 id="整体模型的训练"><a href="#整体模型的训练" class="headerlink" title="整体模型的训练"></a>整体模型的训练</h3><p>由于RPN和后面的检测模型fast r-cnn的特征提取是共享的，因此需要特别的训练方法。</p>
<p>首先训练RPN，然后使用RPN的建议区域训练fast r-cnn，然后使用fast r-cnn的参数初始化共享的特征提取器fine tuneRPN独有部分，最后保持特征提取器固定fine tune fast r-cnn独有的部分。</p>
<p>RPN的输出经过NMS和top-k筛选。</p>
<p>细节：超出图片范围的anchor不参与训练。</p>
<h2 id="Retinanet"><a href="#Retinanet" class="headerlink" title="Retinanet"></a>Retinanet</h2><p>这篇文章的模型架构其实已经比较脱离了r-cnn系列的范畴，但和前几篇一样都是Facebook AI研究院的作品，思想上有很多共同的地方。本文发表于2018年，在yolov3之前。</p>
<p>背景：检测器模型有单阶和双阶的。此前的r-cnn系列都是双阶的，即先提取建议区域，然后进行精细化的坐标和类别预测。单阶检测器直接在原始图像上进行目标的检测识别。此前双阶检测器精度要优于单阶检测器。本文认为其原因主要在于正负样本的不均衡问题，显然多数情况下目标区域仅占整幅图像的一小部分，即单阶检测器难免遇到大量样本都是负样本（背景），少数样本是正样本（目标）。本文从损失函数的构造上解决了这个问题。</p>
<h3 id="focal-loss"><a href="#focal-loss" class="headerlink" title="focal loss"></a>focal loss</h3><p>本文针对样本不均衡问题提出一个新的损失函数，称为focal loss：</p>
<script type="math/tex; mode=display">
\mathrm{FL}\left(p_{\mathrm{t}}\right)=-\alpha_{\mathrm{t}}\left(1-p_{\mathrm{t}}\right)^{\gamma} \log \left(p_{\mathrm{t}}\right)</script><p>其中：</p>
<script type="math/tex; mode=display">
p_{\mathrm{t}}=\left\{\begin{array}{ll}{p} & {\text { if } y=1} \\ {1-p} & {\text { otherwise }}\end{array}\right.</script><p>作为对比，传统分类的交叉熵损失函数为：</p>
<script type="math/tex; mode=display">
\mathrm{CE}(p, y)=\mathrm{CE}\left(p_{\mathrm{t}}\right)=-\log \left(p_{\mathrm{t}}\right)</script><p>显然，本文提出的损失函数多了两项系数。</p>
<p><img src="/2019/09/15/detector-papers/focalloss.svg" alt="focalloss"></p>
<p>y ∈{1,-1}，代表样本为正样本或负样本。P∈[0,1]为神经网络预测认为该样本属于正样本的概率。对于神经网络计算的pt值较大的样本（如例如大于0.7），可以认为网络对于该样本的分类有着较高的置信度，此类样本对于网络模型来说较容易分类，因此对模型的训练实际意义较小。而pt值较小的样本对训练意义更大，尤其是pt值小于0.5的样本（代表网络没能对其进行正确分类）。而从上图可以看出（即γ=0的曲线），传统的交叉熵损失函数在pt值较大时仍然有着不小的损失值，而此类“容易分类”的样本往往占据全部样本的多数，因此会更多地影响训练时梯度下降的方向，使网络无法集中学习“更难分类”的样本特征以提高分类精确度。另外，由于所有样本对损失函数的贡献度完全相同，当负样本数远大于正样本数时，损失函数面临同样的问题，即无法有效从少量的正样本中学习分类特征。由上所述，传统的交叉熵分类方法面临两个问题，一是正负样本不均衡所导致的网络模型难以有效学习目标特征的问题，二是“难易”样本不均衡导致的不能有效对少量困难样本特征进行学习的问题。</p>
<p>改进后focal loss公式的核心在于对样本进行加权，使不同样本对损失函数的贡献不同。相比传统的交叉熵损失函数，该损失函数添加了两项加权系数。αt与pt的定义类似，正样本取α，负样本取1-α，α∈ [0,1]可以作为网络的超参数，通常与正样本出现频率反相关，即正样本出现频率越低，对损失函数贡献加权越高，相应地负样本加权越低。(1-pt)^γ项可以根据样本分类难度进行加权。由前所述，  值高的样本属于容易分类的样本，对应的该项加权系数越低，pt值低的样本属于更难分类的样本，因此加权系数更高，对损失函数的贡献也更高，因此训练过程中，在权重参数更新时，受到该样本的影响更大。γ作为神经网络模型的超参数，可以调节该项系数对损失函数的影响大小。上图为αt取1时不同γ下损失函数曲线。在训练过程中，实际使用的γ取值为2，α=0.25。  </p>
<h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>模型架构方面，特征提取器使用了更加先进的 Feature Pyramid Network，基本结构基于resnet。提取了5层不同尺度的特征图，每层特征图上使用绝对大小相同的anchor，对应到原图上即5种尺寸的感受野。基于特征图，使用了两个全卷积的子网络，一个用于输出分类，另一个用于输出anchor到真值的Bbox回归参数。其他方面，IOU门限同之前一样选取双门限，分别为0.1和0.5。0.5以上被标记为正样本，0.1以下为负样本。</p>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/09/13/matplotlib-learn/" rel="next" title="matplotlib学习">
                  <i class="fa fa-chevron-left"></i> matplotlib学习
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/09/16/国奖/" rel="prev" title="国奖">
                  国奖 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#yolo系列"><span class="nav-text">yolo系列</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#yolov1"><span class="nav-text">yolov1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型输出的概率建模"><span class="nav-text">模型输出的概率建模</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网络架构"><span class="nav-text">网络架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练"><span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预测"><span class="nav-text">预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#局限性"><span class="nav-text">局限性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对比"><span class="nav-text">对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yolov2"><span class="nav-text">yolov2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#改进"><span class="nav-text">改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练-1"><span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数"><span class="nav-text">损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yolov3"><span class="nav-text">yolov3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#改进-1"><span class="nav-text">改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数-1"><span class="nav-text">损失函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#R-CNN系列"><span class="nav-text">R-CNN系列</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#R-CNN"><span class="nav-text">R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#背景及整体框架"><span class="nav-text">背景及整体框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型"><span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#区域建议"><span class="nav-text">区域建议</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征提取"><span class="nav-text">特征提取</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预测-1"><span class="nav-text">预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练-2"><span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#可视化分析与结构分析"><span class="nav-text">可视化分析与结构分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#附：关于Bounding-box-regression"><span class="nav-text">附：关于Bounding-box regression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fast-R-CNN"><span class="nav-text">Fast R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#架构"><span class="nav-text">架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多任务损失函数"><span class="nav-text">多任务损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其它技巧与结论"><span class="nav-text">其它技巧与结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Faster-R-CNN"><span class="nav-text">Faster R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RPN"><span class="nav-text">RPN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#anchors"><span class="nav-text">anchors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RPN的损失函数"><span class="nav-text">RPN的损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#整体模型的训练"><span class="nav-text">整体模型的训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Retinanet"><span class="nav-text">Retinanet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#focal-loss"><span class="nav-text">focal loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型架构"><span class="nav-text">模型架构</span></a></li></ol></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">点点点</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span>
        
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
        
      </div>
    
  </nav>



      </div>
      <div id='mynotice'>
        <p>幸福往往是摸得透彻</br>而敬业的心，却常常隐藏 </p>
      </div>
      <!-- 仓鼠轮子咕咕咕 -->
      <div className="sidebarMouse"><object type="application/x-shockwave-flash" style="outline:none;"          data="https://cdn.abowman.com/widgets/hamster/hamster.swf?up_bodyColor=f0e9cc&amp;up_feetColor=D4C898&amp;up_eyeColor=000567&amp;up_wheelSpokeColor=DEDEDE&amp;up_wheelColor=FFFFFF&amp;up_waterColor=E0EFFF&amp;up_earColor=b0c4de&amp;up_wheelOuterColor=FF4D4D&amp;up_snoutColor=F7F4E9&amp;up_bgColor=F0E4E4&amp;up_foodColor=cba920&amp;up_wheelCenterColor=E4EB2F&amp;up_tailColor=E6DEBE&amp;"          width="220" height="160"><param name="movie"           value="https://cdn.abowman.com/widgets/hamster/hamster.swf?up_bodyColor=f0e9cc&amp;up_feetColor=D4C898&amp;up_eyeColor=000567&amp;up_wheelSpokeColor=DEDEDE&amp;up_wheelColor=FFFFFF&amp;up_waterColor=E0EFFF&amp;up_earColor=b0c4de&amp;up_wheelOuterColor=FF4D4D&amp;up_snoutColor=F7F4E9&amp;up_bgColor=F0E4E4&amp;up_foodColor=cba920&amp;up_wheelCenterColor=E4EB2E&amp;up_tailColor=E6DEBE&amp;"><param name="AllowScriptAccess" value="always"><param name="wmode" value="opaque"></object></div>
      <!-- 网易云音乐播放器 -->
      <div id="music163player">
        
          <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=298 height=263 src="//music.163.com/outchain/player?type=0&id=2975856754&auto=0&height=430"></iframe>
          </iframe>
       
      </div>


    </div>

  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.4.0</div>

        






  
  <script pjax>
  function leancloudSelector(url) {
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = visitors.getAttribute('id').trim();
      var title = visitors.getAttribute('data-flag-title').trim();

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .then(() => {
                leancloudSelector(url).innerText = counter.time + 1;
              })
            
              .catch(error => {
                console.log('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.log('Failed to create', error);
                });
            
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return element.getAttribute('id').trim();
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.url;
            var time = item.time;
            leancloudSelector(url).innerText = time;
          }
          for (var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=XFIHYV4CFEAcTi4Gc7ILy71K-MdYXbMMI')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'XFIHYV4CFEAcTi4Gc7ILy71K-MdYXbMMI',
            'X-LC-Key': 'KXhevyoqaLmhcRjnhY6omVST',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        const localhost = /http:\/\/(localhost|127.0.0.1|0.0.0.0)/;
        if (localhost.test(document.URL)) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });
  </script>






        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  <script src="/lib/pjax/pjax.min.js?v=0.2.8"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>

<script src="/js/next-boot.js?v=7.4.0"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[pjax], script#page-configurations, #pjax script').forEach(element => {
    var id = element.id || '';
    var src = element.src || '';
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (id !=='') {
      script.id = element.id;
    }
    if (src !== '') {
      script.src = src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  








  <script src="/js/local-search.js?v=7.4.0"></script>













    <div id="pjax">

  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'wudjjYBiK2zRLHwnf2NrIL9Y-MdYXbMMI',
    appKey: '0nqgQV0R4j5MqiGAseKA0UtY',
    placeholder: '随便说点什么吧(记得填一下你的id哦)',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

    </div>
</body>
</html>

<!-- 樱花 -->


<!-- 页面点击小红心 -->

  <script type="text/javascript" src="/js/clicklove.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/accioy/live2d-widget@0.4.7/autoload.js"></script>