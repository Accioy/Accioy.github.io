<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2020年工作记录</title>
    <url>/2020/01/25/2020%E5%B9%B4%E5%B7%A5%E4%BD%9C%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h2 id="1-13"><a href="#1-13" class="headerlink" title="1.13"></a>1.13</h2><p>使用numba优化基于python的SAR噪声仿真算法</p><p>重新阅读retinanet源代码</p><p>明天计划：基于pix2pix模型进行简单的实时加噪-去噪训练。</p><a id="more"></a>


<h2 id="1-19"><a href="#1-19" class="headerlink" title="1.19"></a>1.19</h2><p>之前实现了python的加噪类模块。发现亮度有问题，和matlab仿真结果不一样，仔细研究了一下matplotlib对于整形和浮点形、单通道多通道的np数组的处理。</p>
<p>已放寒假，重温c++的内容，为刷面试题做准备。</p>
<p>准备再简单修改一下博客，不那么花里胡哨。</p>
<h2 id="1-25"><a href="#1-25" class="headerlink" title="1.25"></a>1.25</h2><p>最近又比较摸，有两个原因，一是22号下午新电脑到货了，二是过年了，今天是大年初一。最近在为换电脑做迁移。主要是各种数据文件，steam游戏，开发环境。</p>
<p>目前数据文件移的差不多了，steam装了只狼、鬼泣5、伊苏8、血污，新下了naizi战旗，回头康一康(x)。</p>
<p>开发环境配好了py和tf环境，只是简单地在命令行跑了一下demo，还没用vscode做调试试用。</p>
<p>配置好了github和coding的key，杭州服务器因为这几天过年关机了，没发试。</p>
<p>今晚刚做好hexo的迁移，并记录方法。</p>
<p>明天的任务：配好vscode的py和c++调试，继续开始学习。</p>
]]></content>
      <categories>
        <category>Diary or tech</category>
      </categories>
  </entry>
  <entry>
    <title>matplotlib之numpy数组转图片</title>
    <url>/2020/01/19/matplotlib%E4%B9%8Bnumpy%E6%95%B0%E7%BB%84%E8%BD%AC%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<p>最近用matplotlib遇到了一些坑，记录一下。</p><h1 id="图片转数组"><a href="#图片转数组" class="headerlink" title="图片转数组"></a>图片转数组</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">im_file=<span class="string">'test_image.jpg'</span></span><br><span class="line">img=plt.imread(im_file)</span><br><span class="line">print(img.shape)</span><br><span class="line">print(img.dtype)</span><br><span class="line"><span class="comment"># img: numpy array with shape （H，W，c）</span></span><br><span class="line"><span class="comment"># uint8</span></span><br></pre></td></tr></table></figure><a id="more"></a>

<p>如上，类型是uint8的。</p>
<h1 id="数组转图片"><a href="#数组转图片" class="headerlink" title="数组转图片"></a>数组转图片</h1><p>分为以下情况：3通道和单通道，浮点数组和整形数组。</p>
<h2 id="三通道，浮点数组"><a href="#三通道，浮点数组" class="headerlink" title="三通道，浮点数组"></a>三通道，浮点数组</h2><p>三通道的shape是(H，W，C)</p>
<p>对于这种情况，不论原数组取值范围是多少，默认按0-1范围处理，超出范围的直接进行<strong>clip操作</strong>。也就是小于0的数按0（纯黑色）处理，大于1的按1（纯白）处理。</p>
<p>同时会给出警告：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Clipping input data to the valid range <span class="keyword">for</span> imshow with RGB data ([0..1] <span class="keyword">for</span> floats or [0..255] <span class="keyword">for</span> integers).</span><br></pre></td></tr></table></figure>
<p>如果不加<code>cmap=&#39;gray&#39;</code>的话，默认显示热度图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># float array, 3 channels</span></span><br><span class="line"><span class="comment"># For float array with 3 channels, by default the values out of range [0,1] are **Clipped** !</span></span><br><span class="line">x=np.ones([<span class="number">500</span>,<span class="number">600</span>,<span class="number">3</span>])</span><br><span class="line">x*=<span class="number">0.4</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">200</span>):</span><br><span class="line">    x[i]=np.ones([<span class="number">600</span>,<span class="number">3</span>])*<span class="number">9</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">250</span>,<span class="number">300</span>):</span><br><span class="line">    x[i]=np.ones([<span class="number">600</span>,<span class="number">3</span>])*<span class="number">-2</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">370</span>,<span class="number">400</span>):</span><br><span class="line">    x[i]=np.ones([<span class="number">600</span>,<span class="number">3</span>])*<span class="number">0.7</span></span><br><span class="line">print(x.dtype)</span><br><span class="line">plt.imshow(x)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>暂时不贴图了，可以自己试一试效果。</p>
<p>结果应该是灰色背景，从上到下依次是白、黑、浅灰三个横向条带。</p>
<p>如果数组是真实rgb值，建议先归一化到0-1，即<code>x=x/255.</code>。</p>
<h2 id="单通道，浮点数组"><a href="#单通道，浮点数组" class="headerlink" title="单通道，浮点数组"></a>单通道，浮点数组</h2><p>单通道的shape是二维的(H,W)，如果是(H,W,1)会报错。</p>
<p>对于单通道数组，默认进行<strong>归一化</strong>，即原数组中最大值被映射到1，最小值被映射到0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># float array, 1 channel</span></span><br><span class="line"><span class="comment"># For float array with 1 channels, by default all values are normalized</span></span><br><span class="line">x=np.ones([<span class="number">500</span>,<span class="number">600</span>])</span><br><span class="line">x*=<span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">200</span>):</span><br><span class="line">    x[i]=np.ones([<span class="number">600</span>])*<span class="number">200</span></span><br><span class="line">print(x.dtype)</span><br><span class="line">plt.imshow(x,cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果是黑色背景白色条带。</p>
<p>使用<code>plt.imshow(x,cmap=&#39;gray&#39;, clim=(0,255))</code>，即将0作为黑色，将255作为白色处理。</p>
<h2 id="三通道，整形"><a href="#三通道，整形" class="headerlink" title="三通道，整形"></a>三通道，整形</h2><p>默认会对超出0-255的部分进行<strong>clip处理</strong>。即小于0视为0（黑色），大于255视为255（白色）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># int array, 3 channels</span></span><br><span class="line">x=np.ones([<span class="number">500</span>,<span class="number">600</span>,<span class="number">3</span>])</span><br><span class="line">x*=<span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">200</span>):</span><br><span class="line">    x[i]=np.ones([<span class="number">600</span>,<span class="number">3</span>])*<span class="number">900</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">250</span>,<span class="number">300</span>):</span><br><span class="line">    x[i]=np.ones([<span class="number">600</span>,<span class="number">3</span>])*<span class="number">-2</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">370</span>,<span class="number">400</span>):</span><br><span class="line">    x[i]=np.ones([<span class="number">600</span>,<span class="number">3</span>])*<span class="number">200</span></span><br><span class="line">x=x.astype(np.int64)</span><br><span class="line">print(x.dtype)</span><br><span class="line">plt.imshow(x)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="单通道，整形"><a href="#单通道，整形" class="headerlink" title="单通道，整形"></a>单通道，整形</h2><p>默认情况下，最小值映射到0（黑色），最大值映射到255（白色）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># int array, 1 channel</span></span><br><span class="line"><span class="comment"># For int array, by default the array range is mapped to [0,255].</span></span><br><span class="line">x=np.ones([<span class="number">500</span>,<span class="number">600</span>])</span><br><span class="line">x*=<span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">200</span>):</span><br><span class="line">    x[i]=np.ones([<span class="number">600</span>])*<span class="number">175</span></span><br><span class="line">x=x.astype(np.int64)</span><br><span class="line">print(x)</span><br><span class="line">print(x.dtype)</span><br><span class="line">plt.imshow(x,cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>同上，如果数组本身是真实灰度值，使用<code>plt.imshow(x,cmap=&#39;gray&#39;,clim=[0,255])</code>处理。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>matplotlib读取jpg图片时，默认是uint8类型的numpy数组。</p>
<p>在将numpy数组转图片显示时，浮点形默认处理范围是0-1，整形默认处理范围是0-255。</p>
<p>对于三通道数组，超出范围的进行clip处理，对于单通道数组，默认将数组范围线性映射到对应类型的处理范围。</p>
]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title>0202年到了</title>
    <url>/2020/01/09/0202%E5%B9%B4%E5%88%B0%E4%BA%86/</url>
    <content><![CDATA[<p>0202年了。</p><p>好久没搞博客了，写一写9012年的总结吧。</p><p>又到了一下笔就突然忘记想说什么的时候。</p><h1 id="先回收一下去年发在pyq的flag吧"><a href="#先回收一下去年发在pyq的flag吧" class="headerlink" title="先回收一下去年发在pyq的flag吧"></a>先回收一下去年发在pyq的flag吧</h1><a id="more"></a>


<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=276294&auto=0&height=66"></iframe>
> 1.健康生活（包括不限于不熬夜不吃垃圾食品适度运动
>
> 3.摆脱失眠（不用药的情况下

感觉今年唯一养成的好习惯是早起吧（指9点前。熬夜方面的话，今年去校医院找六院来的神经科专家看了，开了安眠药，但并没有吃。正好苏陌夏（网名就不打码了吧）也说这种东西不吃的好。但褪黑素是一瓶又一瓶。另外偶然发现实验室另一个选手也有同样的问题，包括失眠还有心理状况的问题我们两都很像，但他好像比我更严重点。经他提醒说注意高血压，于是观察了几天发现是真的有。去三院看了心内科的专家，开了降压药，但我自己吃了一段时间也停了。另外还顺便查出了胆囊结石，心累。

我的失眠感觉主要还是自己的问题，下决心调整的话还是可以调整的。不过现在身体已经被调校到可以接收一周平均每天睡5小时的地步了。

饮食方面的话，感觉还行吧。运动是不可能运动的，每天上班早上9点到晚上10点半以后，不可能运动。

> 2.与人交流

我以为我懂了，其实我还是不懂。

> 4.看完德川家康
>
> 5.看完基督山伯爵
>
> 11.忍耐（各种意义

德川家康早看完了，而且已经把好不容易学会的忍耐忘得差不多了。

ppp说，为什么要忍，发泄出来，生活会过的很好。

我说，有时忍一忍会过的更好。

“确实。”

很遗憾今年没看其他什么文学作品，估计接下来也不会看了。有那时间精力不如看面试题。

> 6.提高三种外语水平

全 面 退 步 。

其实上次群聚餐去过纯K后，我想慢慢学一些日语歌。不过基本只局限于想。

英语其实也还行吧，今年也看了不少文献，只是到现在也没再刷一次六级。

> 7.去现场看爱豆

虽然没看我南，但今年有了新的单推对象：Ayasa。（我不是dd!

而且还在现场买了cd，然后遇到了早就预料到的没有光驱的问题。虽说买cd基本是为了收藏吧。

> 8.脱单（梦里

确实。

不过今年为这个事情有做一些尝试，不过结果是导致抑郁加重就是了。

> 9.通仁王、魂一重置等等
>
> 10.看看明年有什么新作

仁王说实话比较失望，主要是太无聊，入这个主要是看经常被拿来和黑魂对比，但说实话我觉得比魂3差远了。魂一重置还没玩，今年确实有不少有意思的新作。（让我回忆一下今年都玩了啥）3A大作方面鬼泣5很不错，非3A血污很好玩，死亡细胞其实也还行，只狼买了没玩，fex link比较失望，虽然战斗系统比前作好了不少，但剧情是真的垃圾，前作至少剧情很不错。隐形守护者也有点意思。

好久没打开steam了现在界面变了好多。

另外没想到的是，今年我竟然买了NS，不是巨硬的xbox也不是大法的ps4，而是任天堂的switch。确实放在以前我自己也不会相信。目前主要玩了风花雪月和荒野之息，都很不错。

> “算了，装进心里吧”
>
> “反正都是一些沙雕事”

我也忘了是什么了，很遗憾。

# 补充一点之前的事情

## ayasa演奏会

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1337942134&auto=0&height=66"></iframe>

<p>懒得单独开一篇了，这里顺便说了吧。</p>
<p>去重庆前一天晚上，去看了ayasa的演奏会，这次我也终于是vip票了，有钱真好。顺便在现场买了cd，因为没有光驱现在还没拆过。结束之后还有亲送伴手礼的环节，小姐姐亲手送海报，已经贴在宿舍墙上了。顺便说了一句“阿里嘎多”和“塔诺西”，小姐姐惊喜地问霓虹语哇噶路？反应了一秒后回答了一句“ Sukoshi”。</p>
<p>最开始的时候是在b站fgo的那个什么晚会上看到的，记得当时小姐姐作为嘉宾拉了色彩，瞬间就被圈粉了，太漂亮了，拉小提琴的样子是真的帅。然后才知道在b站有官方帐号，都是acg小提琴曲录像，还外加cos，我当场死亡（x。这次零距离见面发现真的是本人比视频照片里更好看。</p>
<h1 id="关于未来"><a href="#关于未来" class="headerlink" title="关于未来"></a>关于未来</h1><p>总之目前的打算是先刷题，但下学期还有很多课就很烦躁。还有毕设要做。目前已经联系好了Orange的实习，明年6月的，做深度学习图像方面，但明年春招，三四月那会准备再看看，如果有更好的的话还可以选一下。然后在秋招之前争取做好找工作的准备。目前最大的问题首先是文章，感觉顶会已经与我无缘了，能有机会再水一篇普通EI会议就不错了。然后就是跟时间赛跑了，刷题刷代码。秋招先找工作，如果实在是状况恶劣的话就准备申请CSC去法国读三年博，就这样。</p>
<p>就不立什么flag了，明年秋招之前能把这些事全做好就不错了。</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title>重庆游记</title>
    <url>/2019/12/11/%E9%87%8D%E5%BA%86%E6%B8%B8%E8%AE%B0/</url>
    <content><![CDATA[<p>写完突然想起，来重庆前一天的Ayasa live也应该写一写……下次吧。</p><p>这篇post写作时间跨越了好久，从第一次动笔到写完大概跨了20天。</p><a id="more"></a>

<p>EI会议论文，来重庆开会了。会议叫ICSIDP，嘛反正不重要。大致是雷达和信号处理这方面的。被老师催出来一篇学术垃圾投了，然后就中了。能来重庆公款旅游一趟还挺好的。</p>
<p>我们是昨晚到的。实验室好多人来。达叔这边我们4个一起订的高铁票，不让订机票说不给报销（虽然机票更便宜（然后其他老师手下的都订了机票，嗦不出话。。</p>
<p>酒店是我订的，市中心江景豪华双床房，就在长江边上，窗外夜景很漂亮（反正一晚也才280倒是也不贵（反正报销。然后其他一些后订的师兄师姐也订在同一个酒店了。。。点点点。。。</p>
<p>江景房窗外的江景：</p>
<p><img src="/2019/12/11/重庆游记/QQ20191221002957.jpg" alt="QQ20191221002957"></p>
<p>12小时高铁也是挺累的，第一晚到了就早睡了。</p>
<h1 id="第一日"><a href="#第一日" class="headerlink" title="第一日"></a>第一日</h1><p>（时间不多，简单记一点吧）</p>
<p>这天是2019年12月11日。</p>
<p>第二天一早8点达叔就来消息了，在酒店吃了早点就去会场了。逛了一圈。会议也是在一个酒店，是一个郊区的究极豪华酒店。上午简单听了一会报告，在酒店附近到处转了一下。这个酒店还是挺高级的，观景台风景很优美，重庆的山景笼罩着雾气。虽然已经12月中旬了，但天气一点都不冷。中午还下了一会小雨。我们在会议的霓虹logo合了影。</p>
<p><img src="/2019/12/11/重庆游记/QQ图片20191221003157.jpg" alt="QQ图片20191221003157"></p>
<p>会议送了个背包、u盘（16g但不是3.0接口）、1w毫安时的充电宝，本体上自带m-usb和type-c线，遗憾的是自身充电口是m-usb。在会场吃了一堆零食甜点，午餐吃了自助。下午听了达叔那场报告。顺便给群里的研一的学弟学妹们直播了一小下达叔报告的身影23333。听完（其实没听懂）（其实压根没注意听）（就算注意听也听不懂）达叔的报告后，我们和达叔又一起在会议logo合了影。达叔：后面几场报告都很有意思的，xxx可以听一下。我内心：去去去赶紧溜，回市中心开始旅游了。事实上也确实是这样。马上我们就打了两个车直接去解放碑了。</p>
<p>到解放碑大概下午4点（？）。在碑前一起合了影。然后在那附近的步行街逛，边逛边买小吃吃（晚上曾博士溜去见熟人了）。</p>
<p>重庆这个城市的确很赛博朋克，确实。高低楼错落有致，现代与古风混搭，还有壮丽的江景。可惜自己语文不好，只能感叹一句：真令人感叹。</p>
<p>解放碑：</p>
<p><img src="/2019/12/11/重庆游记/QQ图片20191221003201.jpg" alt="QQ图片20191221003201"></p>
<p>我们从解放碑一直走到洪崖洞，风景是真不错。之后去了朝天门。有可爱的学姐同行真好啊。只可惜学姐已经马上毕业了，我离毕业也不远了。</p>
<p>洪崖洞挂着很多许愿符。</p>
<p><img src="/2019/12/11/重庆游记/IMG_20191211_182527.jpg" alt="IMG_20191211_182527"></p>
<p>看着一堆漂亮的许愿符，学姐问我：</p>
<blockquote>
<p>“要许个愿吗？”</p>
<p>“好像没什么好许的。”</p>
<p>“不许愿脱单吗？”</p>
<p>“。。。也是”</p>
</blockquote>
<p>感觉我的孤独传说好像在实验室传开了。。。</p>
<p>想了想，我又说：</p>
<blockquote>
<p>“不过这种事情还是得靠自己争取啊。”</p>
</blockquote>
<p>毕竟自己的幸福是要靠自己争取的，不论是男生还是女生。</p>
<p>洪崖洞买了一个纪念品。</p>
<p><img src="/2019/12/11/重庆游记/QQ图片20191221003205.jpg" alt="QQ图片20191221003205"></p>
<p><img src="/2019/12/11/重庆游记/IMG_20191211_183604.jpg" alt="IMG_20191211_183604"></p>
<p>话说回来，明明是景区物价还真是便宜，也不知是不是我垃圾手游之类的玩多了。买了一个铜（也许是铜吧）的小尺子，10厘米，一端有漂亮的透明半圆球，里面有兰色的花饰。5块钱像是白送。还有摔碗酒，在一家店的外面摆着，完全没人监督，全靠自觉扫码，一碗5块。体验一下也不错。我正要喝然后摔的时候，学姐：等等等等让我先拍个照^_^。</p>
<p>可能这是我人生至今最幸福的时光了吧，唯一的遗憾的是还没有对象，还是孤独一人，还是在努力坚持着，不断消磨着一个人活下去的勇气。我真的会拥有更美好的幸福吗。我真的配拥有更美好的幸福吗。</p>
<p>重庆的她晚上约我了，明天下午见，正好都有空。这么主动倒是出乎我的预料。第一次见面，希望我不要留下遗憾，这一次，我。</p>
<blockquote>
<p>我所馈赠的礼物</p>
<p>纵然全是无形之物</p>
<p>唯独希望 能在你心中一隅</p>
<p>化作璀璨的明星</p>
</blockquote>
<p>——出自初音未来的《歌虽无形》</p>
<h1 id="第二日"><a href="#第二日" class="headerlink" title="第二日"></a>第二日</h1><p>单独行动这天。和我同住的那位曾博士（和我同一级他转博了）一早就回家去了，他家离这里很近，昨晚就告诉我今早回家，明天直接到会场和我们见面。</p>
<p>早上9点半起床，完美错过酒店早餐。</p>
<p><del>不想写了</del></p>
<p>（现在是14号晚上8点51，正在回北京的高铁上。）</p>
<p>第二日起的比较晚，先在酒店楼下简单地吃了点东西，好像是抄手吧。然后自己决定去哪。我让同来的两女生别跟我，毕竟没有计划，我这人究极随缘，何况下午还有约。</p>
<p>坐一号线去了磁器口古镇。</p>
<p>有一说一，重庆这个究极3D地形，地图步行导航究极不靠谱，只有平面，几乎没有z轴，也无法确定你z轴上到底在哪一层。不过那些阶梯小巷子倒是也挺有意思的。</p>
<p>反正就这样走着去了地铁（其实应该叫轻轨？）站。</p>
<p>然后就在古镇古街边走边吃，路边一堆卖小吃和特产的店。不得不说重庆人真的很热情，出租车司机都主动跟你介绍打卡点，然后在古镇白嫖尝了几杯洱茶，即使我明明一点想买的意思都没有，还是被劝着喝了不少。不过我不是很喜欢喝那种酸甜味的茶。还是比较喜欢偏苦的，比如苦荞。</p>
<p><img src="/2019/12/11/重庆游记/QQ图片20191221003609.jpg" alt="QQ图片20191221003609"></p>
<p>古镇人很多，明明是淡季。还有表演，忘了叫啥了反正是拿一个壶嘴究极长的茶壶跳舞（原谅本人的没文化）。不过观看要收费。当时没想到第二天就在另一个地方看了究极高档的专场表演，里面也有这个节目。还看到了类似莎木3里那种落球赌博，正好之前看了一段秦川直播。其中有赌的下面的随机结果竟然是赌2020年脱不脱单。。。嗦不粗话。。。</p>
<p>零距离看了一家魔术小店表演，表演了两个圆环套在一起，圆环可以拿起来随便检查。还有两块心形海绵，给你手里攥一块，表演者手里攥一块，然后表演者手里的突然消失了，你手里出现两块。还有手里的东西消失然后出现在口袋里。我看了两边，也检查了道具，总之是什么问题都没看出来（废话，你随便能看出来人家还吃不吃饭了）。</p>
<p>另外就是采耳店。似乎是这一带的传统文化之一。采耳店挺多的。前一天在洪崖洞也遇到过。以前只在ASMR看过听过采耳，终于有机会实地体验了。大概逛了一圈累了之后找了家采耳店。其实另一个原因是我耵聍一直挺多的感觉，我自己又不常清理。尤其是几年前那次清理弄出血后，左耳一直没完全好，有微微耳鸣，而且里面有东西，很容易堵。</p>
<p>采耳店的阿姨虽然没有ASMR那些播主小姐姐漂亮，但还是挺专业的。成功地帮我取出了左耳里塞了好几年的耳结石，看到那块结石感觉我左耳这么久以来听力竟然没受到影响简直奇迹。不过收费也真是贵，清理50取耳结石150，总共花了200软妹币。还好爷这学期拿了奖学金不缺钱。</p>
<p>有一说一也真够巧，临回去的时候竟然碰巧碰到了另外两个女生来这边逛。。。</p>
<p>大概在磁器口逛到下午3点打车回酒店歇了。晚上5点多接近6点的时候联系了她。</p>
<p>说起来离和她在贴吧认识已经过去好久了。是在学校的贴吧认识的。那还是我在贴吧上非常活跃时候的事。后来好像在QQ上聊过吧，看到了她空间的自拍。记得当时用的还是同一型号的手机，当时我还是安基。。。唉，很多事情已经几乎不记得了。QQ的聊天也早已经消失在网络数据的海洋中了。</p>
<p>后来，很后来的时候，有一天她回了我的个人贴吧发的贴，于是加了微信，还被她发现了我的微博。成为了少数知道我微博的人之一。重新看到了她在pyq和微博的自拍，隐约唤起了模糊的记忆。</p>
<p>当对一个人有想法的时候，就再也不能和对方像以前一样聊天了。</p>
<p>可能我只适合进行不可能的单恋与暗恋，永远在阴影里钦慕这耀眼的太阳。</p>
<p><del>（高铁到了，下次再写吧。</del></p>
<p><del>除了这天外，还有第三日与回程。</del></p>
<p><del>希望我的记忆不要凋零，在我记下之前，这一次，我一定要留住。）</del></p>
<p>在解放碑附近的地铁站见面，通过照片认出了她（她的自拍还是蛮真实的）。</p>
<p>决定去朝天门那吃火锅。边走边聊一些有的没的。她也在一个压抑的环境中，抱怨世事艰难。</p>
<p>路上被她问：</p>
<blockquote>
<p>你是不是不好意思啊</p>
</blockquote>
<p>我还是不擅长这种事啊。</p>
<p>（其实我越来越感觉我是双向障碍了，这个时候处在低落阶段）</p>
<p>她说重庆话的样子还蛮酷的。</p>
<p>我们去朝天门附近吃了火锅。人生第一次挑战吃完全的辣锅，不过由于害怕还是姑且点了鸳鸯锅（听说点鸳鸯锅会被本地人鄙视，然后我果然感觉好像被她白眼了QAQ）。向她请教了重庆火锅的吃法，果然和北方的火锅完全不一样啊（最大的区别，没有面酱）。然后因为吃的太少被吐槽“你战斗力不行”（但我感觉确实没少吃啊，又或者这两天旅游吃的太多了，在会议场也是吃的自助）。第一次挑战完全辣锅倒是成功了。之后我以可以报销的理由请客了。</p>
<p><img src="/2019/12/11/重庆游记/QQ图片20191221003604.jpg" alt="QQ图片20191221003604"></p>
<p>然后成功地在回解放碑的路上迷路了（点点点）。晚上8点多在重庆的小巷子里转悠了好久，还一直在上阶梯。最后到解放碑累得不行。重庆步行真的不能指靠地图导航。</p>
<p>再次感叹这城市确实赛博朋克，很喜欢解放碑那一片。</p>
<p>也没提出拍照留念。送她到轻轨站，分别。我自己回到酒店，微信确认他已经回到住处后，一个人在豪华江景房里思考人生。</p>
<p>忘记在群里说了啥了，但总之被厌烦了。也是，有些情感垃圾还是不要倒出来给人看的好。</p>
<p>今晚只有我一个人。</p>
<h1 id="第三日"><a href="#第三日" class="headerlink" title="第三日"></a>第三日</h1><p>今早赶上了酒店的早餐，9点半结束，我大约9点到的酒店的餐厅。很普通的自助早餐，碰巧遇到了学姐。</p>
<p>这天早上TGA颁奖直播，看了一会秦川的转播。别的忘了，但总之只狼TGA年度最佳游戏，想想我steam夏促买的只狼到现在还没怎么碰。</p>
<p>也没什么再逛的心情了。快到中午的时候学姐在群里问谁去会议酒店吃自助啊，正好我也是这么打算的，于是就回应了。比学姐早下楼了几秒钟，结果出酒店门突然想起自己忘带海报了orz（还好想起了我们4个人的海报都在我那房间里）。</p>
<p>跟着学姐打车去了。</p>
<p>大约12点差10分的时候到会议的酒店，时间正好。</p>
<p>感觉学姐吃的比我都多哈哈，学姐竟然还试图煽动我多吃甜点，这种东西我根本无……………………法拒绝&gt;_&lt;。</p>
<p>吃完等了一会之后另外几位也来了。</p>
<p>之后到了我们的poster场，我们去贴了海报，出现一个意外是，海报是按编号按位置贴的，然后曾博士的位置莫名其妙不见了233333。然后拜托同伴给我和我的海报合了影，然后终于在pyq发了计划已久的一条动态：《垃圾及其学术垃圾》。</p>
<p>海报结束后去最后一场dalao报告学习（摸鱼）。倒是听到了一个研究关于深度学习内部机理的报告，还是涨了一点知识。报告结束后，Y教授给曾博士改他的SCI论文23333。（那个教授是这场会议的终极dalao之一，加拿大来的）。而且是魁北克的，说法语，之前他来学校这边的时候陪过他几天，只不过他的魁北克口音的法语和英语都让我orz。。。</p>
<p>最后就是晚宴了，菜说实话很普通，但有节目表演，有川剧变脸，还有各种舞蹈什么的，以及之前说过的茶壶表演，都是当地的文化，也不知在场的国际友人能不能欣赏得来。</p>
<p>表演结束后，作为会议主办方代表，北理的副校长挨个桌子敬酒。我们这一桌全是我们low学生2333。</p>
<p>晚宴结束已经8点多了，回酒店歇了。第二天一早高铁回京。师姐他们坐飞机，酸酸酸。。</p>
<p><img src="/2019/12/11/重庆游记/QQ图片20191221003208.jpg" alt="QQ图片20191221003208"></p>
<p><img src="/2019/12/11/重庆游记/IMG_20191213_182644.jpg" alt="IMG_20191213_182644"></p>
<h1 id="归程"><a href="#归程" class="headerlink" title="归程"></a>归程</h1><p>在酒店这几天倒是一点都没失眠，虽然以防万一还是每晚褪黑素一片。不过这几天真的好爽。除了重庆的出租车每次都搞得我有点晕车，以及……</p>
<p>曾博士当晚就被叫第二天去实验室了真惨2333，尤其是第二天是周日。</p>
<p>不管怎么说归程了。</p>
<p>北京的出租倒是一点都不晕。</p>
<p>另外相比之下，北京真是没有大城市的感觉。</p>
<p>以及第二天下单了索尼的降噪豆。</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title>miku未来有你演唱会</title>
    <url>/2019/11/17/miku%E6%9C%AA%E6%9D%A5%E6%9C%89%E4%BD%A0%E6%BC%94%E5%94%B1%E4%BC%9A/</url>
    <content><![CDATA[<hr><p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="450" src="//music.163.com/outchain/player?type=0&id=2922328447&auto=0&height=430"></iframe></p><hr><p>昨晚去看了初音的未来有你live，爽到啊。虽然是普票视角一般。之前还有点担心，毕竟去年出过事而且这次通知仍然是不允许站call，我自己不站倒是无所谓但不可能保证所有观众的素质，而且不站着打call看个激霸日式live，既然去了不把自己搞到精疲力竭站不起来不就亏了嘛——这是我看live的信念。好在5点看到日场的人反馈可以站call没人管，皆大欢喜。我是夜场7点到9点的。自带的call棒没买周边（最近消费有丶多。姑且拍了一张纪念照……等我从手机传一下。</p><a id="more"></a>



<p><img src="/2019/11/17/miku未来有你演唱会/IMG_20191116_183014.jpg" alt="IMG_20191116_183014" style="zoom:67%;"> </p>
<p>希望有生之年能去看一场魔法未来……或者fS的infinite synthesis……不知南酱还能不能等到我……</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title>检测识别问题中的metrics</title>
    <url>/2019/11/14/%E6%A3%80%E6%B5%8B%E8%AF%86%E5%88%AB%E9%97%AE%E9%A2%98%E4%B8%AD%E7%9A%84metrics/</url>
    <content><![CDATA[<p>之前一直记不熟各种指标的具体计算，本文准备彻底搞定这个问题，涵盖目前遇到过的所有评价指标。</p><h1 id="TP，TN，FP，FN"><a href="#TP，TN，FP，FN" class="headerlink" title="TP，TN，FP，FN"></a>TP，TN，FP，FN</h1><a id="more"></a>
<p>首先是true-false和positive-negative这两对词。以二分类为例：</p>
<p>positive和negative指的是预测的分类是正样本还是负样本，true和false指的是预测结果是对的还是错的。</p>
<p>因此：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">实际类别\预测类别</th>
<th style="text-align:center">正样本</th>
<th style="text-align:center">负样本</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">正样本</td>
<td style="text-align:center">TP</td>
<td style="text-align:center">FN</td>
</tr>
<tr>
<td style="text-align:center">负样本</td>
<td style="text-align:center">FP</td>
<td style="text-align:center">TN</td>
</tr>
</tbody>
</table>
</div>
<p>基于这些数值可以计算各项指标：</p>
<h1 id="Accuracy-precision-recall等"><a href="#Accuracy-precision-recall等" class="headerlink" title="Accuracy, precision, recall等"></a>Accuracy, precision, recall等</h1><h2 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h2><script type="math/tex; mode=display">
A C C=\frac{T P+T N}{T P+T N+F P+F N}</script><p>分子为所有<strong>正确预测</strong>了的样本数目（即正样本被预测为正，负样本被预测为负），分母为所有样本总数。</p>
<p>Accuracy代表了<strong>所有样本中，被正确预测的样本所占的比例</strong>。</p>
<p>其缺陷在于，当正负样本严重不均衡的时候无法反映出模型的真实水平。</p>
<h2 id="Precision"><a href="#Precision" class="headerlink" title="Precision"></a>Precision</h2><script type="math/tex; mode=display">
P=\frac{T P}{T P+F P}</script><p>分子为正样本被预测为正的个数，分母为所有被预测为正的个数。</p>
<p>Precision代表了<strong>被预测为正的样本中，真的是正样本的比例</strong>。又叫查准率。</p>
<h2 id="Recall"><a href="#Recall" class="headerlink" title="Recall"></a>Recall</h2><script type="math/tex; mode=display">
\operatorname{Recall}=\frac{T P}{T P+F N}</script><p>分子为正被预测为正，分母为正样本的总数。</p>
<p>Recall代表了<strong>所有正样本中，被正确预测为正样本的比例</strong>。</p>
<p>Recall一般被称为<strong>召回率</strong>，，又叫Sensitivity，又叫查全率 ，它和一些传统方法中所说的<strong>正检率</strong>是一个概念。</p>
<h2 id="Specificity"><a href="#Specificity" class="headerlink" title="Specificity"></a>Specificity</h2><p>上述的Sensitivity对应的有Specificity：</p>
<script type="math/tex; mode=display">
\operatorname{Specificity}=\frac{T N}{FP+TN}</script><p>代表实际负样本中被预测为负的比例。</p>
<h2 id="f1-score"><a href="#f1-score" class="headerlink" title="f1-score"></a>f1-score</h2><script type="math/tex; mode=display">
\mathrm{F} 1=\frac{2 * P * R}{P+R}</script><p>f1-score是对precision和recall的调和平均值。</p>
<h1 id="正检率，虚警率，漏警率"><a href="#正检率，虚警率，漏警率" class="headerlink" title="正检率，虚警率，漏警率"></a>正检率，虚警率，漏警率</h1><p>正检率（P_d）：正样本被正确检测到的概率，<strong>即recall</strong>。</p>
<p>漏警率（P_m）：正样本没有被预测为正的概率，即1-P_d，即1-recall。</p>
<p>虚警率（P_f）：负样本被预测为正的概率，即1-Specificity。</p>
<h1 id="指标间的关系"><a href="#指标间的关系" class="headerlink" title="指标间的关系"></a>指标间的关系</h1><p>为了更简单清楚的说明，用P和N表示预测结果是正和负，用P^和N^表示正样本和负样本。不用*号的原因是我不想在markdown里打一堆转义符。我们有P+N=P^+N^。</p>
<p>对于一个正常的识别模型，P中大多数应当是P^，小部分是N^；同样N中大多应当是N^，小部分是P^。</p>
<p>Precision和recall是互相矛盾的一对指标。直观地理解，precision为所有P中P^的比例，要想提高这个比例，需要更加严格的筛选，更严格的筛选意味着，模型只会将那些预测概率非常高的样本预测为P，显然这会更多地筛选掉实际为N^的P，剩下的P中P^。而对于recall，recall代表P^中P的比例，更加严格的筛选会导致，一些P^的样本没有被预测为P，从而导致recall下降。</p>
<p>同理，正检率与虚警率成正相关关系。</p>
<h1 id="ROC曲线与PR曲线"><a href="#ROC曲线与PR曲线" class="headerlink" title="ROC曲线与PR曲线"></a>ROC曲线与PR曲线</h1><h2 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p>ROC曲线即纵轴为正检率，横轴为虚警率。</p>
<p>懒得放示意图了，简单描述一下，一般为从左下到右上的凹函数（注意凹函数的定义），区间为(0,0),(1,1)的正方形区域内。评估方法为计算曲线下积分面积，即AUC。</p>
<p>对于瞎猜模型，ROC曲线应当是一条从(0,0)到(1,1)的对角直线，故AUC为0.5。</p>
<p>对于一个正常的模型，AUC应当在0.5到1之间。越大越好。</p>
<h2 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h2><p>类似地，PR曲线纵轴为precision，横轴为recall。</p>
<p>PR曲线一般为从左上到右下的凹函数，区间同样在(0,0),(1,1)的正方形区域。评估方法同样为计算曲线下积分值，称为AP值，对于多个分类，取平均，称为mAP值。</p>
]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title>需要的感觉</title>
    <url>/2019/10/31/let-it-go/</url>
    <content><![CDATA[<p>Let it go，说的简单，你能做到吗</p><p>我在想是不是我对自己的要求太高了。客观来说，本来就是个垃圾玩意，你再怎么努力也很难脱离这个垃圾范畴。</p><a id="more"></a>

<p>前天，啊已经是前天的事了么。好害怕记忆淡却，好害怕重要的记忆淡却。选择不吃那个安眠药可能是对的。</p>
<p>杨导人真的很好，作为同为你系又选了同系导师的博士学姐，告诉了我很多她了解的情况以及她的经验，很感谢她。</p>
<p><strong>在国外，两个人错把互相需要的感觉当作爱情。</strong></p>
<p>我还在纠结读博的问题，所以这天来和她，我们这一级的导员谈了谈。</p>
<p>刚好这天班委开会，我就顺便混入<del>装作</del>班委，反正也就几个人大家几年来也都认识。</p>
<p>这次班委会我才知道，原来需要帮助的人就在我们身边，也许你身旁的人就在崩溃的边缘<del>比如我自己</del>。</p>
<p><del><strong>如果没有绝对的信念，就不要。</strong></del></p>
<p>班委会的时间比预计的长，我和杨导谈的时间也比预计长了很多。会前杨导在班委群里说晚来十几分钟。我和大班长最先到，我就和他顺便聊了聊，他也在考虑读博的事。会后就和我一起留下来听杨导谈了。还好不然我一个人又要有压力了。</p>
<p><em><del>与人交往的每一秒都使我感到痛苦。</del></em></p>
<p>那句话是谈到国外读博的时候她随口提到的。后来觉得这句话有丶意思，和沙雕群友提起。ppp说，爱情难道不就是互相需要吗。才发现，真是一语惊醒梦中人。</p>
<p>我所追求的东西……</p>
<p>不普通？不可替代？超越一般人？Do great things?或者是某种注定的 Destiny ？</p>
<p><del>（第三条出自HP1，今天不知为什么突发奇想重温了一下HP7里奥利凡德给Harry他们鉴定抢来的魔杖以及HP1奥利凡德卖给Harry凤凰芯魔杖）</del></p>
<p><del>（第四条出自正在听的BGM，鲁鲁修里的c.c.角色歌 Masquerade ）</del></p>
<p><del>（也真是巧，这两个桥段都有命运的感觉）</del></p>
<p>都不是，或者说，都不准确。一直以来我也说不清我自己在追寻什么。直到此刻被点醒。</p>
<p>是啊，我连被需求的感觉都不知道，又怎么会知道什么是爱情呢。</p>
<blockquote>
<p>進む君と止まった僕の</p>
<p>縮まらない隙を何で埋めよう</p>
<p>まだ素直に言葉に出来ない僕は</p>
<p>天性の弱虫さ</p>
</blockquote>
<p>这个歌词里，把你我换下位置，怕是正适合我哦。</p>
<p>不断前进的我和停下的你。我抛弃了太多东西在后面，包括作为一个正常人的感觉。你又在哪里呢。什么都无法直率地说出来的我，是天性的胆小鬼。</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title>加密测试</title>
    <url>/2019/10/24/%E5%8A%A0%E5%AF%86%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="也许有一天我会给你钥匙，也许那一天永远不会到来" data-whm="ERROR">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="我若敞开心扉，你是否有接受的勇气？" />
    <label for="hbePass">我若敞开心扉，你是否有接受的勇气？</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="d6ea1a77151bec94ce9689e14aa25f4d81d055f5d3103158d88e1b67637a9775">bdce5015998c4ed71a66a223a6e1ccaaf2f285ce43984065104d9c74179b2bc1192772268b27c25c0bcd96b24fa9d01ce449ba4d2806220db1364822db16079b</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
  </entry>
  <entry>
    <title>hexo一键自动部署</title>
    <url>/2019/10/22/hexo%E4%B8%80%E9%94%AE%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h1 id="一键"><a href="#一键" class="headerlink" title="一键"></a>一键</h1><p>主要方法是写成命令行脚本然后直接运行，但不知为啥，我写成bat文件运行没有效果。首先目前win10（以前不知道）运行shell脚本时自动进入脚本所在路径的，因此理论上不用写cd，但不知道为什么bat就是不行。</p><a id="more"></a>
<p>不过我平时部署都是powershell做的，主要是比cmd好看一些，虽然我还有更强大的mobaxterm就是了。查了一下powershell的脚本格式是.ps1（也不知道为什么叫这个）。</p>
<p>以下是脚本：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Start-Transcript</span> <span class="string">"path-to-log\log.txt"</span> -force</span><br><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br><span class="line"><span class="built_in">Stop-Transcript</span></span><br></pre></td></tr></table></figure>
<p>Start-Transcript和<code>Stop-Transcript</code>会记录中间的命令行输出，并保存到log文件中，缺点是不记录命令本身。未找到更好的解决方案。</p>
<p>本来想基于命令行变量每次部署建新的日志文件根据时间命名文件，但不知为什么不行。于是就这样了。</p>
<h1 id="自动"><a href="#自动" class="headerlink" title="自动"></a>自动</h1><p>目前的想法是windows计划任务，定时定期执行脚本，目前还没试。</p>
<p>其实发现还有个更好的方法是使用 <a href="https://travis-ci.com/" target="_blank" rel="noopener">Travis CI</a> 托管实现自动生成部署，改天有时间研究一下。</p>
]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title>words of la vie</title>
    <url>/2019/10/22/words-of-la-vie/</url>
    <content><![CDATA[<h1 id="其一"><a href="#其一" class="headerlink" title="其一"></a>其一</h1><p><strong>人，的见识，就像一个集合，集合越大，边界就越大，就越感觉自己无知，</strong></p><p>↑ 来自高中的数学老师</p><p>↓ 是我自己的补充</p><a id="more"></a>


<p>就越可能感觉痛苦。什么都不懂的无知者，往往比事先看透一切的人更加幸福。</p>
<p>那么下辈子，你是否愿意做一个只知道吃和睡的猪？</p>
<h1 id="其二"><a href="#其二" class="headerlink" title="其二"></a>其二</h1><p><strong>成长的过程是痛苦的。</strong></p>
<p>↑  来自本科毕设指导老师oyp老师</p>
<p>↓ 来着看过的某篇博文</p>
<p><strong>Knowledge和skill是一个螺旋上升的感觉。有时感觉技能到位了，但知识水平不到位，有时则相反。</strong></p>
<p>↓ 我的补充</p>
<p>不过很多时候是二者都不到位，所以很痛苦。</p>
<h1 id="其三"><a href="#其三" class="headerlink" title="其三"></a>其三</h1><p>趁着还在这里，抓紧时间把该做的事情做完，不要等毕业之后才后悔。</p>
<p>↑ 来自现在带我的老师</p>
<p>↓ 我的补充</p>
<p>很有道理，所以能不能别给我安排杂活了？</p>
<p><del>↓ 另一次他说</del></p>
<p><del>杂活始终是有的。</del></p>
<p><del>。。。</del></p>
<p><del>行吧。</del></p>
<h1 id="其四"><a href="#其四" class="headerlink" title="其四"></a>其四</h1><p>摸了，下次再说</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title>无题</title>
    <url>/2019/10/22/20191922/</url>
    <content><![CDATA[<p>又是在二院打（hua）工（shui）的一天。</p><p>突然通知就来了，没想到的通知。Un mail pour ceux qui veulent faire une thèse…</p><a id="more"></a>

<p>命运总是不给我准备的时间，不给我充分思考的时间，不给我喘息的余地，我一直在被那只看不见的手强行推着向前，沿着一条不知通向哪里的路。</p>
<p>所以很羡慕哔，早早地安排好了自己的命运。不过说到底还是自己太菜，没进入哔的学院。</p>
<p>我发现一个问题，自从我拿了国奖之后，再说和别人说我自己菜，都没人信了。我感觉很害怕。</p>
<p>不说了，该开始安排开题报告和文献综述了。</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title>She</title>
    <url>/2019/10/22/she/</url>
    <content><![CDATA[<p>Is she a taker, or a giver?</p><p>What are you, then? A giver, or a taker?</p><a id="more"></a>

<p>…</p>
<p>Perhaps everyone is a … faker. </p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>she</tag>
      </tags>
  </entry>
  <entry>
    <title>blog时间到</title>
    <url>/2019/10/18/blog%E6%97%B6%E9%97%B4%E5%88%B0/</url>
    <content><![CDATA[<p>我也不知道要说什么，但总之感觉想写blog</p><p>顺便说一句，双屏真好，有钱真好。我为什么没早点整一个呢，反正一块屏幕也不贵，体验是真的爽，再加一个罗技的无线键盘，虽然也没追求什么机械的，但不亏是专业的手感真爽（<del>比自带电脑的键盘爽多了</del>）。现在可以在宿舍一个屏幕看视频/直播/动画/音乐/你群，另一个屏幕搞coding/写文档/写blog/干正事（？）。不过虽然买这个屏幕的最初主要目的是ns主机模式，电脑外接显示屏只是次要的。以后工作了一定要让单位给我在办公室整个高素质大屏（怕是想屁吃）。</p><a id="more"></a>

<p>昨天晚上微信加了她，不出意外地聊失败了。。。efunweiufnwEINVAIUGFnfawoenfaefnbaij</p>
<p>也怪我，犹犹豫豫的性格还是改不了，做事情过于深思熟虑错过机会造成误解什么的教训也已经不是一次两次了，虽然感觉这几年已经改了不少了但关键时候还是犯错啊。总喜欢把事情完全掌握在手里把一切可能性都预料到，但等算出来的时候已经错过机会了。</p>
<p>又有时候，因为自己思考的太过超前把别人远远甩开了，然后看着其他人一步步思考尝试自己早已预料到的死路（不是思路没打错）。有时因为这样觉得团队拖后腿干脆单干什么的。有时不理解为什么那么明显的事还要自己刻意讲明白之类的。</p>
<p>所以总之我觉得今晚不适合听夜或者梦的歌单了，我现在在放昼的，但这几个歌单也好久没维护了，应该把新的……等等，这么说起来我忘记给学弟发文献让他看了，shit，今天吃饭的时候说好给他发一些文献的，额……</p>
<p>桥豆麻袋我去给他发了先，现在是晚上11点半</p>
<p>然后安顿他一定要理清思路做好笔记看不懂就去找网上别人写的分析文章等等（<del>我真是一个认真负责的学长啊</del></p>
<p>不过这也要怪……算了还是不说了</p>
<p>说起来老师今天问我要不要去xx学校（他在那边上课，女生多），大概叫我去做一些助教工作。我忘记我之前是否有在blog里写过这个事了（大概没有吧。事情是这样的：</p>
<blockquote>
<p>秦川之前有一段非常有意思的直播，那个录像我反复看了几次太好笑了，内容大概：</p>
<blockquote>
<p>雪山狼人杀，秦川是好人，捡到了一个狼人对讲机，对着对讲机频道喊，喂喂喂，有人在这个频道吗，有人在这个频道吗。耳鸣星：我啊。</p>
</blockquote>
<p>有一天，大家一起写一个项目申请书，老师突然问：你们谁还没有女朋友啊。我：我啊。</p>
<p>其他人一片谜之沉默。</p>
</blockquote>
<p>点点点。。。</p>
<p>总之大概就是这样，但他去那边上课的时间刚好和我自己这学期仅有的两门课中的一门时间冲突，今天他又提到这个事情，我说我那个时间有课啊，他说：逃了。</p>
<p>点点点，怎么还有教唆学生逃课的老师的，举报了（x。</p>
<p>咳咳，总之我会尽力让学弟少走一些弯路，希望他能比我更快的成长起来。。但我当初毕竟是对DL还是有一些基本概念的基础的，他起点可能比我低一点（可能也未必吧。</p>
<p>另外真是什么东西都敢拿出来卖钱啊，容天那个服务器管理软件做的什么激霸玩意，感觉完全就是一不成熟的东西拿出来卖。搞的我现在也对GUI界面深恶痛绝了，给我个终端命令行多好。</p>
<p>后悔之前没有写个<em><u>北航学院路小区夜间游荡指北</u></em>了，我其实想写个这玩意来着，但一直没有……好吧就是我懒，少玩点风花雪月大概就写出来了。写出来明天说不定就可以给她看了（<del>虽然她未必感兴趣就是了</del></p>
<p>唉我一直觉得我的……情感爱好？……和正常人不一样，我再怎么表演也难以届到真实的感情，当我试图表达“这难道不moving吗，我已经究极deeply moved了”的时候，别人只会感到：这人有病吧。（也许？</p>
<p>太过感性</p>
<p>这是我认为我与一般理工科男生不同的地方。</p>
<p>我觉得现在很多人缺乏敬畏之心，不过说实话，很多人连尊重都不懂更别说敬畏了。还好我没那样的朋友（因为本来就不会和那样的人做朋友。不过话说回来，你有朋友吗</p>
<p>安凉给我推荐了一个学长的微信，在法国的，不过这事等有后续再说吧</p>
<p>天气这周转凉了，真不是时候啊，这周还下了几天雨。</p>
<p>雨啊……你知道我又想起那个Bgm了。雨中的真实（？</p>
<p>其实我一直没加她的原因有很多，这些天正好赶上项目结题，又赶上另一个合作起步，还要在Boss的安排下去八院打工。</p>
<p>草说起去八院这个事我又想吐槽了，那天我和wwf一早去了被鸽了一上午，下午才开始干活。我们几个是错位轮换着去的，每周周二，也就是第一周12去，第二周23去，第三周34去。他上次去过了，我问他需要带啥，他说：带着灵魂。</p>
<p>（草这个markdown不能emoji，就算能放到网站上去也不支持，不知博客园那边是否支持，也不知能不能通过代码实现）</p>
<p>嘛，迫真灵魂。总之我大概用了十几分钟看了那个matlab分析代码，然后下午提取了所有xx的ROC曲线，补充写了AUC计算的代码并算出了所有ROC曲线的AUC并标注到了图片里。</p>
<p>这工作真扯淡我要死了，总之下次让下一个选手看代码写报告，我宣布挂机。（又不给钱，还tm得我自己倒贴地铁钱草</p>
<p>总而言之明天要见她，虽然微信没聊得来但……希望见面能聊得来吧。</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>she</tag>
      </tags>
  </entry>
  <entry>
    <title>给我一首歌的时间</title>
    <url>/2019/10/16/%E7%BB%99%E6%88%91%E4%B8%80%E9%A6%96%E6%AD%8C%E7%9A%84%E6%97%B6%E9%97%B4/</url>
    <content><![CDATA[<p>夜深了，该睡了</p><p>就用一首歌的时间写这篇吧</p><p>上周还是上上周？已经记不清了，时间的概念已经模糊，总是是国庆最后一天，和你群群友聚餐，然后K歌，纯K。</p><a id="more"></a>


<p>我唱了什么来着</p>
<p>总之是唱了点究极经典老歌吧（比如女人花，不是我点的，我没点过歌，都是别人点的，遇到会唱的就跟着混两句。</p>
<p>顺便说一下开场第一首，届不到的爱，迫真开幕雷击。</p>
<p>我</p>
<p>突然不知道该说什么了，总是这样，要写的时候突然又没什么心情了</p>
<p>说说现在在听的歌吧，也是那次K歌的时候想起的，想起我高中在英语课上唱过，今天偶然想找回来听一听，竟然发现网易云没这首歌的版权。查到歌手后，发现那首歌虽然没版权，但在本地竟然是存在的。打开那个文件夹，我落泪了。2011年……修改日期。这都9012年了，当时我还没有自己的电脑，到现在我自己的电子设备都换了多少代了……这些老物竟然还在，激动的我去网易云创了个怀旧歌单。</p>
<p>发现真正到了这个人生阶段才开始懂想家的感觉。</p>
<p>想在K上唱来着，又想这么多年没听了也唱不好，又很忧伤的歌，算了。</p>
<p>想起了点想说的东西，有人问我当时为什么没唱浮夸。确实突然发现这首歌的歌词挺适合我的，我的表演，你看吗。希望你不要忘记我，但你又在哪里呢。我回答说，不会粤语，不唱的。</p>
<p>I am loving living every single day but sometimes I feel so.</p>
<p>想在一首歌内写完果然是不太可能的，单曲循环了三四次吧。</p>
<p>我现在还喜欢我的every single day吗，当初也许是，现在可能要打上一个大大的问号了。sometimes I feel so倒是真的。</p>
<p>睡了，今天在航天二院搞了一整天，晚上还上课，明天还要去实验室上班。</p>
<p>歌名叫far away from home，歌手Groove Coverage。</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title>blog网站更新记录</title>
    <url>/2019/09/26/blog-update-log/</url>
    <content><![CDATA[<h1 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h1><p>hexo+Next主题</p><p>添加背景图片：自定义_data/styles.styl</p><p>enable主题功能：</p><a id="more"></a>


<ul>
<li><del>点线动画（canvas_nest）</del></li>
<li>访问统计（leancloud_visitors）</li>
<li>valine评论（数据存储基于leancloud国际版）</li>
<li>pjax（需要在主题内安装对应依赖项），现在切换界面不需要重新加载，也就是bgm不会断~</li>
</ul>
<h1 id="评论留言"><a href="#评论留言" class="headerlink" title="评论留言"></a>评论留言</h1><p>评论基于valine，留言基于一个独立page附带评论功能</p>
<p>关闭邮件提醒以及验证码</p>
<h1 id="网易云音乐"><a href="#网易云音乐" class="headerlink" title="网易云音乐"></a>网易云音乐</h1><p>layout/_macro/sideabr.swig</p>
<h1 id="特效"><a href="#特效" class="headerlink" title="特效"></a>特效</h1><h2 id="樱花"><a href="#樱花" class="headerlink" title="樱花"></a>樱花</h2><p>主题配置添加sakura，添加source/js/sakura.js文件，修改css属性z轴设为-1，使其不遮挡post等，layout/_layout.swig添加代码</p>
<h2 id="点击"><a href="#点击" class="headerlink" title="点击"></a>点击</h2><p>主题配置添加clicklove，添加source/js/clicklove.js文件，layout/_layout.swig添加代码</p>
<h1 id="说说界面"><a href="#说说界面" class="headerlink" title="说说界面"></a>说说界面</h1><p>找了一下网上的解决方案，感觉都比较麻烦</p>
<p>自己想了一些解决方案</p>
<p>最后采用直接markdown格式写在page的index.md里面，感觉还行，当然分条点赞评论之类的就不指望了（本来也很难实现</p>
<h1 id="live2d"><a href="#live2d" class="headerlink" title="live2d"></a>live2d</h1><p><del>目前基于：<a href="https://github.com/xiazeyu/live2d-widget-models" target="_blank" rel="noopener">https://github.com/xiazeyu/live2d-widget-models</a></del></p>
<p><a href="https://github.com/stevenjoezhang/live2d-widget" target="_blank" rel="noopener">https://github.com/stevenjoezhang/live2d-widget</a></p>
<p>fork为我自己的repo简要修改，并使用我自己的cdn地址</p>
<p>计划：有时间把api也做成自己的</p>
<h1 id="文章加密"><a href="#文章加密" class="headerlink" title="文章加密"></a>文章加密</h1><p>已添加，并建立了加密文章的模板</p>
<h1 id="公式显示"><a href="#公式显示" class="headerlink" title="公式显示"></a>公式显示</h1><p>参考NEXT官网文档的math equation说明</p>
<h1 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h1><p>参考NEXT官网文档说明。</p>
<p>有个问题是，如果在_data/styles.styl里面修改了.header-inner的样式，搜索框的z轴会出现问题，即使设了z轴9999仍然会被遮挡，审查界面发现搜索框是header-inner的子元素，查了一下说子元素z轴受到父元素制约，因此需要将.header-inner的z轴同样设高。</p>
<h1 id="其他细节"><a href="#其他细节" class="headerlink" title="其他细节"></a>其他细节</h1><p>修改_data/styles.styl侧栏背景色</p>
]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title>乱七八糟的坑以及没什么卯月的知识</title>
    <url>/2019/09/22/%E4%B9%B1%E4%B8%83%E5%85%AB%E7%B3%9F%E7%9A%84%E5%9D%91%E4%BB%A5%E5%8F%8A%E6%B2%A1%E4%BB%80%E4%B9%88%E5%8D%AF%E6%9C%88%E7%9A%84%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h1 id="tenorflow环境要求"><a href="#tenorflow环境要求" class="headerlink" title="tenorflow环境要求"></a>tenorflow环境要求</h1><p>除了对应版本的cuda和cudnn之外，还需要Microsoft Visual C++ Redistributable，安装包大概15M左右，2015到2019版是共享的。</p><a id="more"></a>
<h1 id="git相关"><a href="#git相关" class="headerlink" title="git相关"></a>git相关</h1><h2 id="git-windows下支持中文显示"><a href="#git-windows下支持中文显示" class="headerlink" title="git windows下支持中文显示"></a>git windows下支持中文显示</h2><p>git bash右键options-&gt;text，设置编码为utf-8。</p>
<p>添加环境变量 LESSCHARSET=utf-8 。(git log需要)</p>
<p>设置命令：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">git config --global core.quotepath <span class="literal">false</span>          <span class="comment"># 显示 status 编码</span></span><br><span class="line">git config --global gui.encoding utf-8            <span class="comment"># 图形界面编码</span></span><br><span class="line">git config --global i18n.commit.encoding utf-8    <span class="comment"># 提交说明编码</span></span><br><span class="line">git config --global i18n.logoutputencoding utf-8  <span class="comment"># 输出 log 编码</span></span><br></pre></td></tr></table></figure>
<h2 id="linux下github配置"><a href="#linux下github配置" class="headerlink" title="linux下github配置"></a>linux下github配置</h2><p>首先设置git用户和邮箱，</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">git config --global user.name 用户名</span><br><span class="line">git config --global user.email 邮箱地址</span><br></pre></td></tr></table></figure>
<p>生成ssh key对，生成时指定邮箱，需要与上述邮箱一致，</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">"你的邮箱"</span> -f ~/.ssh/id_rsa</span><br></pre></td></tr></table></figure>
<p>上传公钥到github个人设置下。</p>
<p>本地git添加key文件，</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ssh-agent bash</span><br><span class="line">ssh-add ~/.ssh/id_rsa_for_git</span><br></pre></td></tr></table></figure>
<h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><p>更新同步远程和本地分支状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git remote update --prune</span><br></pre></td></tr></table></figure>
<h1 id="python多线程与多进程"><a href="#python多线程与多进程" class="headerlink" title="python多线程与多进程"></a>python多线程与多进程</h1><p>多线程要返回值需要自己重写多线程的类添加返回值方法。</p>
<p>用多进程写了那个传说中的sleep排序算法，发现结果不太准，排序有误差，不知道是多进程的问题还是time.sleep的问题。</p>
<p>另外，用vscode远程调试多进程的时候会报错，需要添加一句 <code>multiprocessing.set_start_method(&#39;spawn&#39;,True)</code>，原理不明。</p>
<h1 id="opencv-python问题"><a href="#opencv-python问题" class="headerlink" title="opencv-python问题"></a>opencv-python问题</h1><p>在docker ubuntu里遇到的，import cv2会提示缺库。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">apt-get install libsm6</span><br><span class="line">apt-get install libxrender1</span><br><span class="line">apt-get install libxext-dev</span><br></pre></td></tr></table></figure>
<h1 id="pip换源"><a href="#pip换源" class="headerlink" title="pip换源"></a>pip换源</h1><p>pip install后指定-i，豆瓣源： <a href="https://pypi.douban.com/simple/" target="_blank" rel="noopener">https://pypi.douban.com/simple/</a></p>
<p>永久换源：</p>
<p>linux下，建立~/.pip/pip.conf文件，里面写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https://pypi.douban.com/simple/</span><br></pre></td></tr></table></figure>
<p>windows下，在C:/user/username/pip/，新建pip.ini，内容同上。</p>
<h1 id="py编码问题"><a href="#py编码问题" class="headerlink" title="py编码问题"></a>py编码问题</h1><p>文件头：</p>
<p><code># -*- coding: utf-8 -*-</code></p>
<p>sys.getfilesystemencoding()</p>
<p>sys.getdefaultencoding()</p>
<p>sys.stdin.encoding</p>
<p>sys.stdout.encoding</p>
<p>sys.stderr.encoding</p>
<h1 id="numpy的round"><a href="#numpy的round" class="headerlink" title="numpy的round"></a>numpy的round</h1><p>四舍六入五成双</p>
<h1 id="matlab导出矢量图文件（手动"><a href="#matlab导出矢量图文件（手动" class="headerlink" title="matlab导出矢量图文件（手动"></a>matlab导出矢量图文件（手动</h1><p>在图窗选择文件-另存为，选择svg格式。可以用浏览器打开，也可以正常插入md。</p>
<h1 id="word插入图片的问题"><a href="#word插入图片的问题" class="headerlink" title="word插入图片的问题"></a>word插入图片的问题</h1><p>一般来说是设置上下型环绕，随文字移动，不允许重叠。但这个时候经常遇到让人想砸键盘的问题是这玩意死活就是固定位置不随文字动。一般剪切后再粘贴，粘贴选项选择纯图片，重新设格式可解决。应该是word的bug，2016和2019版都遇到过。Visio之类的带格式的东西的话自求多福吧。</p>
<h1 id="cudnn"><a href="#cudnn" class="headerlink" title="cudnn"></a>cudnn</h1><p>runtime和dev两个都安</p>
<h1 id="lsb-release"><a href="#lsb-release" class="headerlink" title="lsb_release"></a>lsb_release</h1><p>的包名是lsb-release而不是什么redhat</p>
<p>然后还需要<code>apt-get install lsb-core</code></p>
<h1 id="nvcc找不到"><a href="#nvcc找不到" class="headerlink" title="nvcc找不到"></a>nvcc找不到</h1><p>打开~/.bashrc ，添加环境变量export PATH=$PATH:/usr/local/cuda/bin</p>
<p>执行<code>source .bashrc</code></p>
<h1 id="关于图像通道数"><a href="#关于图像通道数" class="headerlink" title="关于图像通道数"></a>关于图像通道数</h1><p>看起来是grayscale的图像不一定就是单通道</p>
<h1 id="Anaconda环境命令"><a href="#Anaconda环境命令" class="headerlink" title="Anaconda环境命令"></a>Anaconda环境命令</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda create -n env_name python=3.x</span><br><span class="line"></span><br><span class="line">conda create -n new_env_name --<span class="built_in">clone</span> env_name <span class="comment">#复制环境</span></span><br><span class="line"></span><br><span class="line">conda env list</span><br><span class="line"></span><br><span class="line">conda activate env_name</span><br><span class="line"></span><br><span class="line">conda env remove -n env_name</span><br><span class="line"></span><br><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>
<h1 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h1><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -it -d -p :22 -v /host/path:/Virtual_host/path   --name=<span class="string">"container_name"</span> image_name /bin/bash</span><br></pre></td></tr></table></figure>
<p>后台运行一个镜像容器并映射任意主机端口到容器22端口，并挂载主机某目录。</p>
<p>注意，看到有资料说，应该指定—user username ，指定user后，避免容器内不再是root，在挂载目录中创建的文件在主机中也不再是root。</p>
<p>（注意：容器停止后，重启启动，分配的端口会变。）</p>
<p>然后docker ps查看容器id和映射端口。</p>
<p>进入容器：<code>docker attach container_id</code>。</p>
<p>用ssh登录前先进容器<code>service ssh start</code>。</p>
<h2 id="nv-docker"><a href="#nv-docker" class="headerlink" title="nv docker"></a>nv docker</h2><p>要运行需要调用pgu的镜像容器，需要加—runtime=nvidia，或者nvidia-docker run。</p>
<p><del>依赖：查了nv-docker的github issue，感觉貌似是容器包含cuda，依赖主机的显卡驱动。</del></p>
<p>没错就是这样，<a href="https://github.com/NVIDIA/nvidia-docker/wiki/CUDA#requirements" target="_blank" rel="noopener">https://github.com/NVIDIA/nvidia-docker/wiki/CUDA#requirements</a></p>
<p>根据这个wiki说明，cuda的镜像启动需要依赖显卡驱动，并且根据镜像内的cuda版本对显卡驱动版本有要求，具体版本对应要求见上述链接。</p>
<p><del>因此容器的cuda对主机的显卡驱动有要求，如果不兼容，要么改主机的显卡驱动版本（没有root不能，而且也没查到非root装显卡驱动的方法，要么就尝试更换与当前主机显卡驱动版本兼容的cuda镜像。</del></p>
<p><del>但目前我用dnndk这种冷门的东西，我又不想自己建一个镜像把xilinx的那些东西自己装一遍，因此尝试更改镜像内cuda版本看看。</del></p>
<p><del>上述尝试失败，和狗管理沟通准备升级显卡驱动，考虑到cuda可以同时装多个版本，并且新版显卡驱动对旧版cuda兼容（如果nv的那个对照表没写错的话），但愿以前的环境不要出现兼容问题。</del></p>
<p>ok事实证明：</p>
<ul>
<li>新版显卡驱动向下兼容旧的cuda，新的cuda必须要新的显卡驱动，cuda可以多版本并存，设置环境变量即可</li>
<li>nvdocker依赖宿主机的显卡驱动，不依赖宿主机的cuda</li>
</ul>
<h2 id="docker时区问题"><a href="#docker时区问题" class="headerlink" title="docker时区问题"></a>docker时区问题</h2><h3 id="方法一：对于dockerfile"><a href="#方法一：对于dockerfile" class="headerlink" title="方法一：对于dockerfile"></a>方法一：对于dockerfile</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">RUN <span class="built_in">echo</span> <span class="string">"Asia/Shanghai"</span> &gt; /etc/timezone</span><br><span class="line">RUN dpkg-reconfigure -f noninteractive tzdata</span><br></pre></td></tr></table></figure>
<h3 id="方法二：同步主机时区"><a href="#方法二：同步主机时区" class="headerlink" title="方法二：同步主机时区"></a>方法二：同步主机时区</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">docker run -v /etc/localtime:/etc/localtime &lt;IMAGE:TAG&gt;</span><br></pre></td></tr></table></figure>
<p>好处是不会修改镜像本身</p>
<h3 id="方法三：运行中的容器"><a href="#方法三：运行中的容器" class="headerlink" title="方法三：运行中的容器"></a>方法三：运行中的容器</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Asia/Shanghai"</span> &gt; /etc/timezone</span><br><span class="line">dpkg-reconfigure -f noninteractive tzdata</span><br></pre></td></tr></table></figure>
<p>适用于不想新建镜像或重启容器</p>
<h1 id="虚拟主机ubuntu"><a href="#虚拟主机ubuntu" class="headerlink" title="虚拟主机ubuntu"></a>虚拟主机ubuntu</h1><h2 id="关于root密码"><a href="#关于root密码" class="headerlink" title="关于root密码"></a>关于root密码</h2><p>是随机的，输入passwd更改，对于ssh，root默认不能远程登录，需要修改配置文件（见ssh章节</p>
<h2 id="关于包"><a href="#关于包" class="headerlink" title="关于包"></a>关于包</h2><p>首先apt-get update更新源，之后才能安装新东西</p>
<p>然后apt-get upgrade更新所有包</p>
<p><strong>包相关命令：</strong></p>
<p>清除无用包：<strong>apt-get clean &amp;&amp; apt-get autoclean</strong> 　　#一般用<strong>apt-get autoclean</strong>就够了</p>
<p>更新源：<strong>apt-get update</strong>　　　　#同步 /etc/apt/sources.list 源的索引，这样才知道是不是有最新的包</p>
<p>更新包：<strong>apt-get upgrade</strong>　　　#更新所有已安装的软件（upgrade只是简单的更新包，不管这些依赖，它不添不删。）</p>
<p>更新包2：apt-get dist-upgrade　#包与包之间存在各种依赖关系。而dist-upgrade可以根据依赖关系的变化添删包。（<strong>依赖有时会出错，建议新手谨慎使用</strong>）</p>
<p>安装软件：apt-get install XXX [—reinstall]　#<strong>—reinstall</strong> 是重新安装包，一般都是用 <strong>apt-get install XXX</strong></p>
<p>修复依赖：apt-get <strong>-f</strong> install　　　　　　　#修复依赖，类似于win里面的缺少dll，-f之后他帮你修复安装一些依赖</p>
<p>删除软件：apt-get remove XXX [—purge]　#—purge 卸载的时候把配置文件也删了，完全删除包可以用这个选项：<strong>apt-get remove XXX —purge</strong></p>
<p>-————————————————————————————————————————————————————————————————————————————</p>
<p>其他命令：(特殊环境下使用)</p>
<p>搜索软件：apt-get search xxx</p>
<p>检查是否有损坏的依赖：apt-get check　　　（偶尔想起来或者软件打不开的时候使用【有时候被清理了】）</p>
<p>获取包信息：apt-cache show xxx</p>
<p>有哪些依赖：apt-cache depends xxx　　　 （比如安装一个包，修复依赖也失败，那就看看有哪些依赖，自己手动安装）</p>
<p>被哪些包依赖：apt-cache rdepends xxx　　（比如卸载一个包的时候，查一下被哪些包依赖）</p>
<p>下载该包源代码：apt-get source xxx</p>
<p>安装相关编译环境：apt-get build-dep xxx</p>
<h1 id="linux命令"><a href="#linux命令" class="headerlink" title="linux命令"></a>linux命令</h1><p>df -h查看硬盘情况</p>
<p>du 目录查看目录大小，-m以MB为单位</p>
<p>nvidia-smi 查看gpu使用情况</p>
<p>tar -zxvf xxx.tar.gz  -C /usr/java 解压到目标路径，参数：</p>
<ul>
<li>c：打包</li>
<li>t：查看包中文件名</li>
<li>x：解压</li>
<li>j：.bz2格式</li>
<li>z：.gz格式</li>
<li>v：显示处理文件</li>
<li>f：指定文件</li>
<li>C：指定目录</li>
</ul>
<p>(顺便说一句，tar是打包没有压缩，gz只能针对单文件压缩，所以才搞了个这玩意)</p>
<p>jobs查看后台列表，但只能查看当前终端的</p>
<p>bg，fg前后切换</p>
<p>nohup远程终端掉了也可以继续执行，信息保存在.out文件里，命令后加 &amp;后台运行</p>
<p>正在运行时ctrl+z切后台</p>
<p>ps -ef可查看全部后台，|grep <em>*</em>可以查找</p>
<p>ctrl+s暂停终端变化，ctrl+q继续（好像是吧</p>
<p>ls -lR|grep “^-“|wc -l<br>查看某文件夹下文件夹的个数，包括子文件夹里的。</p>
<p>复制文件夹：cp xxx /www/wwwroot -r</p>
<p>rsync同步文件，一般-avP即可</p>
<p>rsync通过ssh从一台服务器推送文件到另一台服务器：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">rsync -avz --progress -e <span class="string">'ssh -p 目标服务器端口号'</span> thefile user@ip:/path/</span><br></pre></td></tr></table></figure>
<p>浏览器等界面：在连接X11情况下（Mobaxterm自动连接）命令行输入firefox自动弹出</p>
<h1 id="ssh服务端"><a href="#ssh服务端" class="headerlink" title="ssh服务端"></a>ssh服务端</h1><p>apt-get install openssh-server</p>
<p>启动：/etc/init.d/ssh start</p>
<p>ps -e|grep ssh</p>
<p>默认root不能远程登录，需要修改配置文件</p>
<p>vim /etc/ssh/sshd_config</p>
<p>修改PermitRootLogin项，默认为without-password，修改为yes</p>
<p>配置ssh key登录：</p>
<p>本地生成私钥-公钥对（需要安装git，或者一些ssh客户端也支持ssh key生成）。</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -b <span class="number">4096</span></span><br></pre></td></tr></table></figure>
<p>上述命令会在本地生成一个私钥文件和一个公钥文件。</p>
<p>在服务器用户文件夹下创建文件夹<code>~/.ssh/</code>，把公钥(id_rsa.pub)上传至该文件夹。然后将公钥内容存入该文件中：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">cat id_rsa.pub &gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure>
<p>最后修改文件夹权限：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">chmod -R 700 .ssh</span><br></pre></td></tr></table></figure>
<h1 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h1><p>i插入，esc命令模式，:到尾部输入命令，w保存，q退出，!强制</p>
<p>搜索：/<em>*</em></p>
<p>保存退出 ：:wq!</p>
<p>不保存退出：:q!</p>
<h1 id="mathtype公式显示不全"><a href="#mathtype公式显示不全" class="headerlink" title="mathtype公式显示不全"></a>mathtype公式显示不全</h1><p>调整word行距，不要设固定值，最好改成xx倍</p>
<h1 id="有关random-shuffle函数的局限性"><a href="#有关random-shuffle函数的局限性" class="headerlink" title="有关random.shuffle函数的局限性"></a>有关random.shuffle函数的局限性</h1><p>刚刚踩的坑，记录一下，万一对别人有帮助呢。</p>
<p>random.shuffle千万不要用于二维numpy.array（也就是矩阵）!!!</p>
<p>最好使用np包自带的random。</p>
<p><a href="https://blog.csdn.net/qq_21063873/article/details/80860218" target="_blank" rel="noopener">https://blog.csdn.net/qq_21063873/article/details/80860218</a></p>
<h1 id="关于卡刷img"><a href="#关于卡刷img" class="headerlink" title="关于卡刷img"></a>关于卡刷img</h1><p>用balenaEtcher往SD卡里刷img的时候，空间越刷越小，格式化不能恢复，我qtmd，百度了一下解决方法：</p>
<p><a href="https://blog.csdn.net/tz2101/article/details/55000309" target="_blank" rel="noopener">https://blog.csdn.net/tz2101/article/details/55000309</a></p>
<p>有时候U盘做完系统盘安装后，发现U盘大小大幅度缩水，怎么恢复原大小。<br>步骤如下：<br>插入U盘；</p>
<p>打开“运行”，输入cmd命令，然后输入diskpart，进入diskpart管理；<br>输入list disk，查看目前电脑的硬盘，然后输入select disk 3（原作者说一般是1反正我这里是3）（注意空格，名字是英文，list里显示的是中文还行）；<br>此时已选中U盘了，输入clean，清除U盘上的数据；<br>输入create partition primary ，即创建主分区，active 激活该分区，format quick搜索 快速格式化该分区。</p>
<p>exit退出diskpart管理，exit退出cmd。此时你发现你的U盘恢复了原始大小。</p>
<h1 id="linux命令行搭建shadowsocks代理"><a href="#linux命令行搭建shadowsocks代理" class="headerlink" title="linux命令行搭建shadowsocks代理"></a>linux命令行搭建shadowsocks代理</h1><p>问题在于弄好socks代理还要搞个https转socks，如果有gui的话弄好shadowsocks浏览器有插件可以解决。</p>
<p>命令行情况下，<code>pip install shadowsocks</code>，然后sslocal命令。</p>
<p>linux的shadowsocks服务端和客户端是一个包，服务端命令是ssserver。</p>
<p>问题在于https转socks实现，需要第三方软件实现。</p>
<h1 id="关于主机ping开发板等设备"><a href="#关于主机ping开发板等设备" class="headerlink" title="关于主机ping开发板等设备"></a>关于主机ping开发板等设备</h1><p>直连网线的话主机设静态ip并和开发板ip在同一区段内</p>
]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title>国奖</title>
    <url>/2019/09/16/%E5%9B%BD%E5%A5%96/</url>
    <content><![CDATA[<p>今天国奖答辩，评审，学院5个名额，12个人参加答辩，都是从本科开始就是dalao的选手（除了我，平均成绩都能比我高20分吧（哭。</p><a id="more"></a>
<p>一周前告白失败，我下定决心果酱一定要拼一次，准备了很多材料用来吹（除了成绩。</p>
<p>答辩前大家在休息室等。</p>
<p>还记得放假前来这个休息室交实习材料，夏天，很安静，因为已经放假了，整个楼静悄悄的。休息室里放了桌椅，沙发，明亮的配色，还有微波炉，打印扫描仪。</p>
<p>几个人坐在里面，有说有笑交流了一会，每个人做的方向都不一样，有做nlp的，有做材料的，有做气动的，等等。有从国外回来的，有要出没出成的。不过，只有我一个憨憨做了将近20页ppt，在之前通知答辩只给5分钟的情况下。这一年我有太多东西想展示（其实仔细想想都是些小事，没有什么值得特别夸耀的）。室友说，我只做了5页ppt，有人说，巧了，我只做了6页，还有人说，我也不知道有什么好讲的。大家在屋里聊了一些有的没的，包括国内外一些见闻啦，发文章啦，等等。然后就是互相称呼dalao，又谦虚说自己是来炮灰之类的。直到答辩轮到我们级，顺序是按学号来的，因此先是研三的答辩。</p>
<p>大家一个一个进去，又一个一个离开，最后一个人出来时，我说，等一下，把我的ppt删一下，他立刻反应过来，没问题，我补充说，毕竟上面有的东西还是比较敏感，他说，没问题，我理解，毕竟大家都是xx（学校名）的。</p>
<p>大家走后，我在答辩的会议室外等了一会。</p>
<p>答辩评委有院长，比较让我惊讶的是两个级的导员竟然都是评委，其他人忘了是谁，我一向对这些不关心。也就院长能问出来一些有水平的问题。</p>
<p>等了会后，导员出来了，问结果，说，你先走吧，结果会通知的，你答辩挺好的。又问是今天内出吗，说今天内会出来。</p>
<p>又在其他人都走了的会议室独自等了好一会。</p>
<p>结束之后，帮忙导员收拾了一下会场。</p>
<p>我答辩还没讲到一半的时候，被提醒只剩一分钟。出来之后问其他人，之前也有人被卡了时间。瞬间觉得自己真是太天真，明明预料到了这个风险，竟然愚蠢到没有问前面的人，竟然想着大家时间都这么短，我是不是可以讲多点。迅速讲到大半之后，被喊停，用一句话总结。我翻到最后一页，简要地用几句话讲了自己未来一年的规划，并且表达了想借这次评审希望我的努力得到认可的想法。</p>
<p>导员告诉了我结果。后来又提醒我说你成绩比其他人有不小差距，需要努力。</p>
<p>我发了pong友圈庆祝后，导员说我太高调了，毕竟结果还没公示。（掩面</p>
<p>感觉自己还是太不成熟了啊</p>
<p>很感谢这次导员对我的帮助。以及某位鼓动我申请这玩意的沙雕，让我证明了拿果酱比找女朋友容易。另外还有爽哥的名言。</p>
<p><img src="/images/shuangge.png" alt></p>
<p>想起昨晚听的音乐（第一次在博文中插音乐，不知效果怎么样</p>
<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1364351236&auto=0&height=66"></iframe><br>我在大学的人生最高光的时刻，估计也就是告白那天晚上了吧。</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习图像detector相关论文整理</title>
    <url>/2019/09/15/detector-papers/</url>
    <content><![CDATA[<p>包含r-cnn系列，yolo系列。</p><p>r-cnn系列主要为二阶检测器，包含r-cnn，fast r-cnn，faster r-cnn，以及两个分支，用于实例分割的mask r-cnn（这个就暂时不研究了）以及基于focal loss的1阶检测器retinanet。</p><a id="more"></a>

<p>yolo系列，包含yolov1，yolov2，yolov3。</p>
<h1 id="yolo系列"><a href="#yolo系列" class="headerlink" title="yolo系列"></a>yolo系列</h1><h2 id="yolov1"><a href="#yolov1" class="headerlink" title="yolov1"></a>yolov1</h2><h3 id="模型输出的概率建模"><a href="#模型输出的概率建模" class="headerlink" title="模型输出的概率建模"></a>模型输出的概率建模</h3><p>图片首先被分割为S*S的网格（grid cell）。如果一个bbox的中心落在一个网格里，则该网格负责检测该物体。（对于pascal数据集，S定为7）</p>
<p>每个网格预测B个bbox及其confidence score，confidence定义为Pr(Object)∗IOU。 若该网格内没有物体，score应当是0；否则score被希望等于IOU（即如果网格不包含目标中心，则Pr(Object)=0，否则=1）。这个score反应了置信度，此处置信度是指模型预测的box是否真的包含目标（即第一项）以及模型自己认为box的准确度（即第二项）。</p>
<p>每个bbox包含5个预测值，分别为x,y,w,h和score。(x,y)坐标是box中心相对于网格边界（？），(w,h)是相对于整幅图像。score代预测box与真实值的iou。（iou不是能通过xywh直接算出来吗？）</p>
<p>每个cell同时还预测C个类别概率，即</p>
<script type="math/tex; mode=display">
\begin{equation}
\operatorname{Pr}\left(\text { Class }_{i} | \text { Object }\right)
\end{equation}</script><p>根据条件概率公式，有：</p>
<script type="math/tex; mode=display">
\operatorname{Pr}\left(\text { Class }_{i} \text { |Object }\right) * \operatorname{Pr}(\text { Object }) * \text { IOU }^{truth}_{pred}=\operatorname{Pr}\left(\text { Class }_{i}\right) * \text { IOU }^{truth}_{pred}</script><p>在原文中，对于PASCAL VOC数据集，有20类目标（C=20）,采用S=7，B=2，最终预测为</p>
<script type="math/tex; mode=display">
S \times S \times(B * 5+C)</script><p>即，7*7个网格，每个网格预测B个bbox，每个bbox有5个预测值，分别为xywh和score，以及20个类别概率值。</p>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>基于用于图片分类的googleNet，有一定的调整。两个网络模型，正常（yolo）的和tiny的(fast yolo)，正常的有24个卷积层+2个全连接层，tiny的有9个卷积层，滤波器也更少。使用了dropout在第一个全连接层之后（0.5）。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>imagenet上预训练分类，训练前20个卷积层，后面连了平均池化和一个全连接层。预训练使用的是224*224的输入，正式训练扩大了2倍长宽。使用了几何数据扩充。</p>
<p>输出xywh被归一化至0-1。激活函数使用leaky relu。</p>
<p><strong>虽然每个网格预测两个bbox，但实际训练仅采用与真值IOU较高的那个bbox，即每个网格实际仅有一个参与训练的有效预测。</strong></p>
<p>损失：使用sum-squared error。（和方差，其实和均方差没太大区别，只是没有除样本总数n）</p>
<p>具体包含三部分：</p>
<ol>
<li>Score置信度：标签为Pr(Object)∗IOU，若有物体中心落入该网格内，则Pr=1，否则Pr=0.</li>
<li>xywh：xywh都被归一化，仅在物体中心落入该网格时才在损失函数中出现。</li>
<li>类别预测：对于一个网格，如果物体中心落入，则对应类别label为1，其余为0。同上，仅在物体中心落入该网格时才在损失函数中出现，即此项预测值为物体存在条件下的概率，Pr(class_i | Object)。</li>
</ol>
<p>解决问题：大量不含目标的网格，即正负样本严重不均衡的问题：加权，含目标的加权至5，不含的至0.5。</p>
<p>解决问题：小box的小偏移误差比大box偏移误差更重要：预测wh的平方根而非值本身。</p>
<p>综上损失函数为：</p>
<script type="math/tex; mode=display">
\lambda_{\text {cord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\text {obj }}\left[\left(x_{i}-\hat{x}_{i}\right)^{2}+\left(y_{i}-\hat{y}_{i}\right)^{2}\right]
\\+\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\text {obj }}\left[(\sqrt{w_{i}}-\sqrt{\hat{w}_{i}})^{2}+(\sqrt{h_{i}}-\sqrt{\hat{h}_{i}})^{2}\right]
\\+\sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\mathrm{obj}}\left(C_{i}-\hat{C}_{i}\right)^{2}
\\+\lambda_{\mathrm{noobj}} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\mathrm{noobj}}\left(C_{i}-\hat{C}_{i}\right)^{2}
\\+\sum_{i=0}^{S^{2}} \mathbb{1}_{i}^{\mathrm{obj}} \sum_{c \in \text { classes }}\left(p_{i}(c)-\hat{p}_{i}(c)\right)^{2}</script><p>注意，1^obj_i代表物体中心落在网格i内，1^obj_ij代表网格i的第j个box负责预测该物体。因此该损失函数仅惩罚含有物体中心的网格分类，仅惩罚负责预测那个物体的bbox坐标。</p>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>对大型物体，可能有多个cell同时给出较为准确的预测，可以理解，大型物体可能有好几个cell靠近中心点。使用nms筛选。</p>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p>空间局限性：可预测的数量有限（98个，并且有空间独立）。对小的集群目标性能较差。对异形目标（长宽比）较差。对大小尺寸目标损失函数有问题。</p>
<h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>yolov1出的时候，faster r-cnn已经出了，但好像还没开源（？）,而且这个时候精度还不如fast r-cnn，原文主要对比了fast r-cnn。此外原文还提到了Deformable parts models，Deep MultiBox，OverFeat，MultiGrasp等。</p>
<p>对比faster r-cnn，yolo在pascalvoc数据集上速度更快，但精度较低。</p>
<p>YOLO PASCAL2007+2012 mAP63.4 FPS45 输入448*448</p>
<h2 id="yolov2"><a href="#yolov2" class="headerlink" title="yolov2"></a>yolov2</h2><p>又叫yolo9000，号称可以实现识别9000类目标。基于yolo改进，超越基于resnet的faster r-cnn和ssd并且更快。可以预测没有标签的目标类别。</p>
<p>提出一种新的训练方法，可以实现同时利用检测数据集和分类数据集。</p>
<h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><p>引入BN</p>
<p>改进基于高分辨率的预训练方法</p>
<p>引入anchor boxes：优点：预测补偿（offsets）而非坐标本身，对网络来说更易于学习。移除原来的FC层，使用ancor boxes进行预测。</p>
<p>anchor dimension聚类：不同于R-CNN手动选取anchor box的数量和尺度，本文使用<strong>k-means聚类</strong>计算box dimensions的先验信息。根据训练集所有的boxes使用k-means聚类计算出的k个box尺度即定为anchor的尺度。综合性能和计算速度，取k等于5。</p>
<p>在k-means计算时，距离公式以IOU为标准。公式如下：</p>
<script type="math/tex; mode=display">
d(\text { box }, \text { centroid })=1-\operatorname{IOU}(\text { box, centroid })</script><p>注意：此时的IOU计算仅考虑形状，不考虑位置。</p>
<p>位置预测：为使模型更容易收敛，预测相对于cell偏移值，坐标预测值落于0-1范围内（使用sigmoid激活函数）。</p>
<p>模型对每个特征图像素点预测5个bbox，每个bbox预测5个值。</p>
<script type="math/tex; mode=display">
\begin{aligned} b_{x} &=\sigma\left(t_{x}\right)+c_{x} \\ b_{y} &=\sigma\left(t_{y}\right)+c_{y} \\ b_{w} &=p_{w} e^{t_{w}} \\ b_{h} &=p_{h} e^{t_{h}} \end{aligned}\\\operatorname{Pr}(\text { object }) * I O U(b, \text { object })=\sigma\left(t_{o}\right)</script><p>注意：此处预测仍然是相对于网格。对于416×416的输入，特征图为13×13，即每个特征图像素点对应原图32×32，即一个网格。由于每个cell对应到特征图上为一个像素点，因此坐标预测范围理应在0-1之间。</p>
<p>细粒度特征：yolo在13<em>13的特征图上预测。加了一条passthrough layer带来前一层26\</em>26的特征。通过一个神秘（？）的调整把 26×26×512 的特征图调整到了 13×13×2048 ，然后和原来的特征图进行concatenate。</p>
<p>多尺度训练：模型仅使用卷积和池化层，因此与输入尺度无关。训练模型使其适应多尺度输入。每10个batches随机换一个输入尺度： {320,352,…,608}. </p>
<p>性能：YOLOv2 416×416 2007+2012 mAP76.8 FPS67 </p>
<p>特征提取器：大多检测系统基于VGG16，但这玩意很笨重。本文提出了Darknet-19分类模型，作为Yolov2的基底。</p>
<p>输出：先进行224<em>224分辨率的1000类分类训练，然后在448分辨率fine tune。然后溢出最后一个卷积层，加入一个新的卷积层，有1024个滤波器，每个跟着一个1\</em>1卷积，层数由输出纬度决定。对VOC数据集，预测5个anchors，每个anchor对应4个坐标值、一个置信度、20个类别概率，因此输出为5×（1+4+20）=125层通道。</p>
<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>识别与检测混合训练法：混合二者数据，若图片带有检测框标签，则进行完整的loss计算与反向传播，若图片仅有类别信息，则只根据分类损失进行反向传播。</p>
<p>问题：使用softmax分类损失的数学前提是所有类别是exclusive的，而一般分类数据集标签会有细分之类的问题，并不是exclusive的。针对这个问题，作者建立了层级类别标签树（wordTree），对每一个层级（是互斥的）进行softmax分类。对于预测概率，使用条件概率公式，并且假定每个图片包含object的概率为1。</p>
<p>在检测时，对每个box都预测一个概率树（对应类别树），自顶向下取最大分支直至绝对概率达到某个门限，即预测这个类别。</p>
<p>测试结果表明，这种方法使得模型对仅有类别训练没有位置训练的类型有一定的检测泛化能力。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>看完论文后总觉得缺点什么，原来是没有损失函数，这部分原文里没有提到，函数式从网上抄的。总的来说，损失函数包含了上述内容大多关键点，还是很有必要分析一下的。</p>
<script type="math/tex; mode=display">
\begin{aligned} \operatorname{loss}_{t}=\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} 1_{\text {Max } IOU<Thresh} \lambda_{\text {noobj}} *\left(-b_{i j k}^{o}\right)^{2} \\+1_{t<12800} \lambda_{\text {prior}} * \sum_{r \epsilon(x, y, w, h)}\left(\text {prior}_{k}^{r}-b_{i j k}^{r}\right)^{2} \\+1_{k}^{\text {truth}}\left(\lambda_{\text {cord}} * \sum_{r \epsilon(x, y, w, h)}\left(t r u t h^{r}-b_{i j k}^{r}\right)^{2}\right.\\+\lambda_{o b j} *\left(\operatorname{IOU}_{\text {truth}}^{k}-b_{i j k}^{o}\right)^{2} \\\left.+\lambda_{\text {class}} *\left(\sum_{c=1}^{c}\left(\operatorname{truth}^{c}-b_{i j k}^{c}\right)^{2}\right)\right) \end{aligned}</script><p>W和H为最终特征图的分辨率（13×13），A为每个网格（特征图像素点）的anchor数目，原文取5。λ为各项权重。bo为置信度score，br为box的坐标和大小，bc为类别概率。</p>
<p>对特征图上所有anchors进行遍历：</p>
<p>第一项：若anchor与所有真值的最大IOU小于门限（原文门限取0.6），则计算该项，为no object的损失计算。此时给出的置信度越大则损失越大，即计算置信度与0的L2。</p>
<p>第二项：仅在训练初期（已训练样本数量小于12800时）计算。该项计算的是真值中心点不在对应网格内，但IOU大于门限的anchor，计算预测值与先验box的差距。即希望这种情况网络不进行任何预测。</p>
<p>第三项：真值Box中心落在对应网格内，仅匹配IOU最大的那个先验anchor进行计算，其余的anchors，除非IOU小于门限值被计入第一项，否则直接忽略（这一点类似于yolov1）。计算box与真值的损失，置信度损失（根据yolov1的条件概率公式，置信度与IOU进行L2计算），以及类别概率损失。</p>
<h2 id="yolov3"><a href="#yolov3" class="headerlink" title="yolov3"></a>yolov3</h2><h3 id="改进-1"><a href="#改进-1" class="headerlink" title="改进"></a>改进</h3><p>首先，IOU门限被改至0.5。</p>
<p>分类：使用多标签分类，移除softmax，因为作者发现这玩意对于提升性能并不是必要的。取而代之，使用independent logistic classiﬁers（逻辑回归），训练中使用二分类交叉熵损失。</p>
<p>多尺度特征：使用了3个不同尺度（类似于FPN）。输出仍然包括了bbox，置信度，类别概率。输出维度为N ×N ×[A∗(4+1+C)] ，N为特征图边长，A为每个点的box数量，原文对COCO数据集取3，4为box的4个坐标，1为置信度，C为类别概率向量。除了原框架的最后一层外，使用之前两层进行2倍上采样，并且与更之前的特征图进行融合，从而同时得到有意义的语义和细粒度信息。</p>
<p>Anchor尺度的先验聚类：仍然使用k-means聚类，只不过这次k取9。并且，根据结果的尺度，给三个尺度的特征图进行分配，每个特征图分配三个尺度的anchors。具体分配方式大致为，分辨率较高的特征图分配感受野较小的anchors，分辨率较低的特征图分配感受野较大的anchors，从结果上说这与retinanet基本一致。</p>
<p>更新主干网络：从darknet19更新至darknet53。</p>
<p>性能：从mAP上看，yolov3并没有比retinanet好，但有一说一，这玩意计算速度大约是retinanet的三倍。</p>
<p>特点：yolov3相比前两代在小目标检测上有大幅提升，但对于中型和大型目标有丶差。</p>
<p>作者有试过但没成功的idea：坐标预测，直接预测线性补偿；focal loss：作者认为可能是因为yolo本身有基于条件概率的置信度预测，因此不存在大量背景的负样本造成的不均衡问题；双门限：在faster r-cnn中，IOU大于0.7的被认为是正样本，小于0.3的被认为是负样本（背景），其余被忽略，而在yolo中，这样做并没有得到一个好结果。</p>
<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>原文这次同样没有提到损失函数的具体公式，但根据上述内容不难看出，Yolov3相对于yolov2在损失函数上的变化主要为在分类中使用了逻辑回归和交叉熵损失。根据源码，公式应该如下：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\text { loss (object) }=\lambda_{\text {coord }} \sum_{i=0}^{K \times K} \sum_{j=0}^{M} I_{ij}^{obj}\left[\left(x_{i}-\hat{x}_{i}\right)^{2}+\left(y_{i}-\hat{y}_{i}\right)^{2}\right]+} \\ {\quad \lambda_{\text {coord}} \sum_{i=0}^{K \times K} \sum_{j=0}^{M} I_{j}^{o b j}\left(2-w_{i} \times h_{i}\right)\left[\left(w_{i}-\hat{w}_{i}\right)^{2}+\left(h_{i}-\hat{h}_{i}\right)^{2}\right]-} \\ {\sum_{i=0}^{K \times K} \sum_{j=0}^{M} I_{i j}^{obj}\left[\hat{C}_{i} \log \left(C_{i}\right)+\left(1-\hat{C}_{i}\right) \log \left(1-C_{i}\right)\right]-} \\ {\quad \quad \lambda_{\text {noobj}} \sum_{i=0}^{K \times K} \sum_{j=0}^{M} I_{i j}^{\text {noobj}}\left[\hat{C}_{i} \log \left(C_{i}\right)+\left(1-\hat{C}_{i}\right) \log \left(1-C_{i}\right)\right]} \\ {\sum_{i=0}^{K \times K} I_{i j}^{o b j} \sum_{c \in \text { classes }}\left[\hat{p}_{i}(c) \log \left(p_{i}(c)\right)+\left(1-\hat{p}_{i}(c)\right) \log \left(1-p_{i}(c)\right)\right]}\end{array}</script><p>对于无obj的情况，仅计算置信度，不同于之前的L2，此处采用交叉熵作为置信度的损失函数。同理在有obj的情况，置信度和分类概率都采用交叉熵。对于有无obj的判断原理同之前，只是门限被改至0.5。同样，在每个网格仅有一个anchor即IOU最高的anchor被赋予真值，其余的不参与计算。</p>
<h1 id="R-CNN系列"><a href="#R-CNN系列" class="headerlink" title="R-CNN系列"></a>R-CNN系列</h1><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p>本文发表于2014年。</p>
<h3 id="背景及整体框架"><a href="#背景及整体框架" class="headerlink" title="背景及整体框架"></a>背景及整体框架</h3><p>背景：将CNN在图像分类领域的成功（2012年）应用于目标检测上面。检测问题：一种方式是使用滑窗检测器，即CNN。在当时，在整幅图像上做滑窗检测有技术难题，因此没有采用。而是使用了一种叫recognition using region的模型，在之前被证明有效。在测试阶段，会从图像提取约2000个候选框，并使用CNN进行特征提取，使用afﬁne image warping技术使得CNN的输入维度无视region形状固定，从而使得输出固定，然后使用支持向量机（SVM）进行分类。</p>
<p>另一个问题：在当时，有目标位置标签的数据集还不够多。作者使用无监督学习进行预训练，然后用有监督学习进行fine tune。使用了非极大值抑制筛选（NMS），使用了bounding-box regression算法，并分析证明这个算法在模型中很关键。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>综上，整个模型分为三个部分，第一部分生成候选区域，第二部分为CNN用于从每个候选区域提取一个固定维度的特征向量，第三部分是一个SVM用于分类。</p>
<h4 id="区域建议"><a href="#区域建议" class="headerlink" title="区域建议"></a>区域建议</h4><p>已有很多方法，本文采用的是一种叫selective search的方法。原文没有对该方法进行描述仅给出了相关参考文献。我觉得现在深度学习模型也不用这些东西了，所以就不对它们进行研究了。</p>
<h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><p>不论候选区域的形状大小，一律放缩变形至227×227，CNN有5个卷积层2个全连接层，最终输出为一个4096的向量。</p>
<h3 id="预测-1"><a href="#预测-1" class="headerlink" title="预测"></a>预测</h3><p>预测流程：对图片进行selective search，提取出大约2000个区域，每个变形后输入CNN，提取出特征向量，然后用SVM打分，每一个类别都训练了一个SVM。然后使用NMS筛选，对一堆互相之间IOU超过门限的候选框仅保留最大得分的框。</p>
<p><strong>特别注意：NMS筛选是基于每个类别独立筛选的！这样做的好处在于可以处理重叠（遮挡）的情况，但问题（我实际碰到的）是，如果数据集中有两种或多种较为相似的目标，而得分门限设置又不高的情况下，可能会出现多个IOU极高的检测结果。</strong></p>
<h3 id="训练-2"><a href="#训练-2" class="headerlink" title="训练"></a>训练</h3><p>CNN进行了分类预训练。然后用候选区域进行了分类训练。注意背景也被当作一个类别加入，在候选区域与真值IOU大于0.5的时候被当作正样本，否则为负样本。为了避免不均衡问题，正负样本比例被控制在1:3。</p>
<p>对SVM二分类器的训练，正负样本的定义有所不同，ground truth为正样本，对于负样本的定义，经过IOU上的搜索，门限被定为0.3，即0.3以下被当作负样本。对原数据集的每一个类别训练了一个二分类的SVM。</p>
<p>总的来说，R-CNN训练分为三步，首先基于selective search的建议区域训练CNN分类，然后基于CNN的特征训练SVM分类，然后基于CNN的特征训练bounding box regression。</p>
<h3 id="可视化分析与结构分析"><a href="#可视化分析与结构分析" class="headerlink" title="可视化分析与结构分析"></a>可视化分析与结构分析</h3><p>R-CNN提到了一些可视化分析方法的参考文献，以后有时间可以看一看。</p>
<p>本文里用的方法是，使用大量的建议区域作为网络输入，检查特定单元的激活程度，按照激活程度对建议区域进行排序，并使用NMS筛选。</p>
<p>结构分析发现，在没有fine tune的情况下，移除掉最后两层FC层并没有降低多少mAP，尽管这两层占用了大多数的参数量，CNN的power主要来自于其卷积层。在有fine tune的情况下，最后两层FC层带来的精度提升明显。</p>
<p>CNN结构选择对精度影响明显。</p>
<p>另外，BBox regression是在DPM里提出的，在本文也用于定位。对于一个selective search的区域，使用CNN的特征（移除最后两个FC）训练一个线性回归器，来预测一个新的检测窗。</p>
<p>原文还提到了语义分割方面的迁移应用，不做仔细研究。</p>
<h3 id="附：关于Bounding-box-regression"><a href="#附：关于Bounding-box-regression" class="headerlink" title="附：关于Bounding-box regression"></a>附：关于Bounding-box regression</h3><p>输入：两组box坐标（中心xy和长宽wh），一组为ground truth，记作G，另一组为（前一阶段的）建议值，记作P。该算法的目标是学习一个变形，可以将P变换至G。</p>
<p>对于神经网络来说，首先对输入经过特征提取，然后输出结果。这个过程可以抽象为一组函数，记作d(P)。在这里，原文令：</p>
<script type="math/tex; mode=display">
d_{\star}(P)=\mathbf{w}_{\star}^{\mathrm{T}} \boldsymbol{\phi}_{5}(P)</script><p>其中Φ(P)是神经网络最后一层输出，经过一个线性函数变换得到d(P)。此处这个线性函数我觉得主要是为了进行维度上的变换，将神经网络的输出压缩至一个4×1的向量，另一方面应该是神经网络在上文中主要承担分类方面的特征提取，因此对于位置修正，这里需要额外的参数来训练。对于这些参数的训练方法为普通的L2 loss加正则项：</p>
<script type="math/tex; mode=display">
\mathbf{w}_{\star}=\underset{\hat{\mathbf{w}}_{\star}}{\operatorname{argmin}} \sum_{i}^{N}\left(t_{\star}^{i}-\hat{\mathbf{w}}_{\star}^{\mathrm{T}} \boldsymbol{\phi}_{5}\left(P^{i}\right)\right)^{2}+\lambda\left\|\hat{\mathbf{w}}_{\star}\right\|^{2}</script><p>即令d(P)向t<em>（真实的变换）回归。t\</em>的定义为：</p>
<script type="math/tex; mode=display">
\begin{aligned} t_{x} &=\left(G_{x}-P_{x}\right) / P_{w} \\ t_{y} &=\left(G_{y}-P_{y}\right) / P_{h} \\ t_{w} &=\log \left(G_{w} / P_{w}\right) \\ t_{h} &=\log \left(G_{h} / P_{h}\right) \end{aligned}</script><p>这样定义的原因是：我们需要定义一个变形方式，这个变形方式进行“变换的程度“应当只与P和G的偏移量有关，而与box本身的尺寸形状无关。因此使用上述方式计算偏移的相对量。</p>
<p>根据上式，原始建议区域P经过上述模型的特征提取后输出d(P)，计算修正后的box的公式如下，即对上式进行逆变换：</p>
<script type="math/tex; mode=display">
\begin{aligned} \hat{G}_{x} &=P_{w} d_{x}(P)+P_{x} \\ \hat{G}_{y} &=P_{h} d_{y}(P)+P_{y} \\ \hat{G}_{w} &=P_{w} \exp \left(d_{w}(P)\right) \\ \hat{G}_{h} &=P_{h} \exp \left(d_{h}(P)\right) \end{aligned}</script><p>原文强调了两个注意点：一是优化函数中的正则项很重要，二是如果P和G相差太多，则该算法没有意义。为了保证第二点，事先计算P与G的IOU，大于门限（原文取0.6）才进行上述回归计算。</p>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p>本文发表于2015年。</p>
<p>R-CNN的缺点：计算慢，训练繁琐，由于多步训练特征要被写入硬盘，消耗资源多。它对每一个建议区域都要单独计算。</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>R-CNN从整幅原始图像提取特征。</p>
<p>对于每一个建议区域，使用ROI池化层从特征图中提取一个固定长的特征向量。</p>
<p>每一个特征向量被喂进一系列FC层中，然后传到两个输出层：一个输出softmax分类概率，注意背景也被当作一个类别在其中；另一个输出bounding box回归的4个参数。</p>
<p>ROI池化层：使用最大池化，将任意感兴趣区域的特征转化为一小块有着固定分辨率（H×W）的特征图。方法为将原区域分割为H×W的小块，每一小块进行最大池化，不同通道独立进行。</p>
<p>注：region of interest (RoI)感兴趣区域</p>
<p>综上，整个模型的输入为原始图像以及该图像的感兴趣区域列表，感兴趣区域仍然来着selective search，数量仍然大约为2000，输出为类别分类（包括背景）以及bbox回归值。</p>
<h3 id="多任务损失函数"><a href="#多任务损失函数" class="headerlink" title="多任务损失函数"></a>多任务损失函数</h3><script type="math/tex; mode=display">
L\left(p, u, t^{u}, v\right)=L_{\mathrm{cls}}(p, u)+\lambda[u \geq 1] L_{\mathrm{loc}}\left(t^{u}, v\right)</script><p>其中第一项为分类损失，采用普通的softmax交叉熵损失，公式如下：</p>
<script type="math/tex; mode=display">
L_{\mathrm{cls}}(p, u)=-\log p_{u}</script><p>其中p为预测概率，u为真实类别。再次注意此处分类包含背景类。</p>
<p>第二项为bounding box regression损失，其中v是bbox回归的目标值，t^u为预测值。[u≥1]代表进对与真值box匹配了的box做计算，即对于背景，u=0。λ为损失的权重，原文中取1。</p>
<p>对于bbox回归：</p>
<script type="math/tex; mode=display">
L_{\mathrm{loc}}\left(t^{u}, v\right)=\sum_{i \in\{x, y, w, h\}} \operatorname{smooth}_{L_{1}}\left(t_{i}^{u}-v_{i}\right)</script><p>其中：</p>
<script type="math/tex; mode=display">
\operatorname{smooth}_{L_{1}}(x)=\left\{\begin{array}{ll}{0.5 x^{2}} & {\text { if }|x|<1} \\ {|x|-0.5} & {\text { otherwise }}\end{array}\right.</script><p>相对于R-CNN中采用的L2 loss，作者认为该loss更加稳定。</p>
<p><img src="/2019/09/15/detector-papers/smoothl1.svg" alt="untitled"> </p>
<p>在本文中，采用IOU大于0.5的预选框进行回归计算。IOU在0.1到0.5之间的被当作背景。</p>
<h3 id="其它技巧与结论"><a href="#其它技巧与结论" class="headerlink" title="其它技巧与结论"></a>其它技巧与结论</h3><p>由于ROI池化层结构特殊，原文提到了其反向传播的计算，此处不做深入研究。</p>
<p>探索了两种方法使模型对尺度鲁棒，brute force和 image pyramids。</p>
<p>输出同样进行了NMS筛选，<strong>并且仍然是每个类别独立筛选</strong>。</p>
<p>可以使用truncated SVD对FC层的计算进行加速。</p>
<p>作者证明了多任务系统训练的有效性（因为共享特征表述）。</p>
<p>多尺度训练对精度提升几乎没有效果。softmax分类略优于SVM。</p>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p>本文发表于2016年。</p>
<p>提出了 Region Proposal Network (RPN) （区域建议网络）来替代之前的selective search。实现真正由神经网络完成的端到端的目标检测识别。Selective search运算已经成为之前检查系统的速度瓶颈。</p>
<p>提出anchor boxes的机制。</p>
<h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><p>区域建议网络是一个全卷积网络，因此可以接收任何尺寸的图片作为输入，输出若干建议区域及其score。</p>
<p>首先基于原图由CNN提取特征。然后在特征图上进行n×n的滑窗（原文取n=3），并输入到两个FC层中，一个用于 box-regression 另一个用于分类。两个FC层对全图参数共享，因此实际实现上，结构为在原特征图上进行n×n卷积，然后接入两个并行的1×1卷积。</p>
<h4 id="anchors"><a href="#anchors" class="headerlink" title="anchors"></a>anchors</h4><p>对于滑窗，每一个位置预测k个框。因此box regression输出维度为4k，分类输出维度为2k，注意此时的分类仅仅针对objectness，即仅区分含有目标或者是背景，不进行具体的识别分类。对于位置预测，要进行BBox回归，需要有先验box，此处在每个位置定了9个先验box，3种大小和3种长宽比。即k=9。若特征图大小为W×H，则共有W×H×k个先验框。这样的方式保证了平移不变性。</p>
<h4 id="RPN的损失函数"><a href="#RPN的损失函数" class="headerlink" title="RPN的损失函数"></a>RPN的损失函数</h4><p>由上述可知，RPN的损失包含两项，分别为box回归损失和二分类损失。对于anchor boxes样本的正负判定，条件为：选取与真值IOU大于0.7的box为正样本，或者选取IOU最高的box为正样本。第二条是为了保证正样本一直存在，通常第一条已经足够。注意，一个真值可能匹配到多个先验anchor boxes；与任何真值IOU小于0.3的anchor boxes被定为负样本（即背景）；其余的anchor boxes不参与训练。</p>
<p>该结构还有一个巧妙之处在于，对于不同尺寸和长宽比的先验anchor boxes，计算回归的权重是不共享的。</p>
<p>综上，损失函数如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} L\left(\left\{p_{i}\right\},\left\{t_{i}\right\}\right)=& \frac{1}{N_{c l s}} \sum_{i} L_{c l s}\left(p_{i}, p_{i}^{*}\right) \\+\lambda & \frac{1}{N_{r e g}} \sum_{i} p_{i}^{*} L_{r e g}\left(t_{i}, t_{i}^{*}\right) \end{aligned}</script><p>对于正样本，p<em>=1，对于负样本，p\</em>=0。分类loss采用交叉熵，回归算法仍然采用bounding box regression，loss和之前的模型一样采用smoothL1。权重λ原文设为10，并提到模型对该参数在很大范围内并不敏感。</p>
<p>训练时，控制正负样本比例在1:1。</p>
<h3 id="整体模型的训练"><a href="#整体模型的训练" class="headerlink" title="整体模型的训练"></a>整体模型的训练</h3><p>由于RPN和后面的检测模型fast r-cnn的特征提取是共享的，因此需要特别的训练方法。</p>
<p>首先训练RPN，然后使用RPN的建议区域训练fast r-cnn，然后使用fast r-cnn的参数初始化共享的特征提取器fine tuneRPN独有部分，最后保持特征提取器固定fine tune fast r-cnn独有的部分。</p>
<p>RPN的输出经过NMS和top-k筛选。</p>
<p>细节：超出图片范围的anchor不参与训练。</p>
<h2 id="Retinanet"><a href="#Retinanet" class="headerlink" title="Retinanet"></a>Retinanet</h2><p>这篇文章的模型架构其实已经比较脱离了r-cnn系列的范畴，但和前几篇一样都是Facebook AI研究院的作品，思想上有很多共同的地方。本文发表于2018年，在yolov3之前。</p>
<p>背景：检测器模型有单阶和双阶的。此前的r-cnn系列都是双阶的，即先提取建议区域，然后进行精细化的坐标和类别预测。单阶检测器直接在原始图像上进行目标的检测识别。此前双阶检测器精度要优于单阶检测器。本文认为其原因主要在于正负样本的不均衡问题，显然多数情况下目标区域仅占整幅图像的一小部分，即单阶检测器难免遇到大量样本都是负样本（背景），少数样本是正样本（目标）。本文从损失函数的构造上解决了这个问题。</p>
<h3 id="focal-loss"><a href="#focal-loss" class="headerlink" title="focal loss"></a>focal loss</h3><p>本文针对样本不均衡问题提出一个新的损失函数，称为focal loss：</p>
<script type="math/tex; mode=display">
\mathrm{FL}\left(p_{\mathrm{t}}\right)=-\alpha_{\mathrm{t}}\left(1-p_{\mathrm{t}}\right)^{\gamma} \log \left(p_{\mathrm{t}}\right)</script><p>其中：</p>
<script type="math/tex; mode=display">
p_{\mathrm{t}}=\left\{\begin{array}{ll}{p} & {\text { if } y=1} \\ {1-p} & {\text { otherwise }}\end{array}\right.</script><p>作为对比，传统分类的交叉熵损失函数为：</p>
<script type="math/tex; mode=display">
\mathrm{CE}(p, y)=\mathrm{CE}\left(p_{\mathrm{t}}\right)=-\log \left(p_{\mathrm{t}}\right)</script><p>显然，本文提出的损失函数多了两项系数。</p>
<p><img src="/2019/09/15/detector-papers/focalloss.svg" alt="focalloss"></p>
<p>y ∈{1,-1}，代表样本为正样本或负样本。P∈[0,1]为神经网络预测认为该样本属于正样本的概率。对于神经网络计算的pt值较大的样本（如例如大于0.7），可以认为网络对于该样本的分类有着较高的置信度，此类样本对于网络模型来说较容易分类，因此对模型的训练实际意义较小。而pt值较小的样本对训练意义更大，尤其是pt值小于0.5的样本（代表网络没能对其进行正确分类）。而从上图可以看出（即γ=0的曲线），传统的交叉熵损失函数在pt值较大时仍然有着不小的损失值，而此类“容易分类”的样本往往占据全部样本的多数，因此会更多地影响训练时梯度下降的方向，使网络无法集中学习“更难分类”的样本特征以提高分类精确度。另外，由于所有样本对损失函数的贡献度完全相同，当负样本数远大于正样本数时，损失函数面临同样的问题，即无法有效从少量的正样本中学习分类特征。由上所述，传统的交叉熵分类方法面临两个问题，一是正负样本不均衡所导致的网络模型难以有效学习目标特征的问题，二是“难易”样本不均衡导致的不能有效对少量困难样本特征进行学习的问题。</p>
<p>改进后focal loss公式的核心在于对样本进行加权，使不同样本对损失函数的贡献不同。相比传统的交叉熵损失函数，该损失函数添加了两项加权系数。αt与pt的定义类似，正样本取α，负样本取1-α，α∈ [0,1]可以作为网络的超参数，通常与正样本出现频率反相关，即正样本出现频率越低，对损失函数贡献加权越高，相应地负样本加权越低。(1-pt)^γ项可以根据样本分类难度进行加权。由前所述，  值高的样本属于容易分类的样本，对应的该项加权系数越低，pt值低的样本属于更难分类的样本，因此加权系数更高，对损失函数的贡献也更高，因此训练过程中，在权重参数更新时，受到该样本的影响更大。γ作为神经网络模型的超参数，可以调节该项系数对损失函数的影响大小。上图为αt取1时不同γ下损失函数曲线。在训练过程中，实际使用的γ取值为2，α=0.25。  </p>
<h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>模型架构方面，特征提取器使用了更加先进的 Feature Pyramid Network，基本结构基于resnet。提取了5层不同尺度的特征图，每层特征图上使用绝对大小相同的anchor，对应到原图上即5种尺寸的感受野。基于特征图，使用了两个全卷积的子网络，一个用于输出分类，另一个用于输出anchor到真值的Bbox回归参数。其他方面，IOU门限同之前一样选取双门限，分别为0.1和0.5。0.5以上被标记为正样本，0.1以下为负样本。</p>
]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title>matplotlib学习</title>
    <url>/2019/09/13/matplotlib-learn/</url>
    <content><![CDATA[<p>想仔细学习一下matplotlib这个库很久了，经常会用到但有些地方一直没搞明白，网上找了一些blog感觉写的也不是很系统，于是自己开始啃官方文档</p><a id="more"></a>
<h1 id="那就开始8"><a href="#那就开始8" class="headerlink" title="那就开始8"></a>那就开始8</h1><p>首先是figure类，即一幅图，会追踪所有的Axes对象，如果不是完全空白的图像的话一个figure至少应该含有一个axes.</p>
<p>Axes即一个图像，axes包含了要画图像的数据，一个figure可含有多个axes。一个axes可能含有2个或三个axis，与data limits关联，即 <a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_xlim.html#matplotlib.axes.Axes.set_xlim" target="_blank" rel="noopener"><code>set_xlim()</code></a> and <a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_ylim.html#matplotlib.axes.Axes.set_ylim" target="_blank" rel="noopener"><code>set_ylim()</code></a> <code>Axes</code>methods。每个axes含有一个title，xlabel和ylabel，<code>set_title()</code>以及<code>set_xlabel()</code>。</p>
<p>Axis即数轴，</p>
<p>数据最好是numpy.array格式。</p>
<h1 id="pyplot"><a href="#pyplot" class="headerlink" title="pyplot"></a>pyplot</h1><p>是matplotlib的一个模块，对于这里面的函数，总是存在一个“当前”的figure和axes，被自动或者手动创建。例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, x, label=<span class="string">'linear'</span>)</span><br><span class="line">plt.plot(x, x**<span class="number">2</span>, label=<span class="string">'quadratic'</span>)</span><br><span class="line">plt.plot(x, x**<span class="number">3</span>, label=<span class="string">'cubic'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'x label'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y label'</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"Simple Plot"</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://matplotlib.org/_images/sphx_glr_usage_003.png" alt="../../_images/sphx_glr_usage_003.png"></p>
<p>pylab不被推荐使用。</p>
<p>Figure可由<code>plt.figure()</code>创建，</p>
]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title>todo</title>
    <url>/2019/09/12/todo/</url>
    <content><![CDATA[<p>好好学习一下plt库</p><p>bbox regression和nms算法，及其手动实现</p><p>读ssd和yolo系列的论文，可能rcnn系列也要重读一下</p><a id="more"></a>


<p>手动实现faster-rcnn或者retinanet</p>
<p>另一边，下周前尽量搞定报告和ppt，草好像还有fpga那块，感觉做不完。。</p>
<p>之后：去噪：基于GAN的和传统方法，基于GAN的超分辨，如果能拿到地物分类的数据集的话也可以做一做</p>
<p>重新学习一下底层理论</p>
<p>做几个kaggle</p>
]]></content>
      <categories>
        <category>tech</category>
      </categories>
  </entry>
  <entry>
    <title>告白</title>
    <url>/2019/09/11/gaobai/</url>
    <content><![CDATA[<p>告白的意思是告诉对方你的自白，向对方传递自己内心的想法</p><p>从这个角度来说……我也不知道有没有成功，大概成功了吧</p><a id="more"></a>

<p>但从一般意义上来说，</p>
<p>发个贴纪念一下自己第一次告白以及第一次告白失败</p>
<p>于2019-09-11晚</p>
<hr>
<h1 id="某日编辑"><a href="#某日编辑" class="headerlink" title="某日编辑"></a>某日编辑</h1><p>虽然说是告白，只是确认一下对方有没有想法而已，遗憾的是对方并没有，所以还没开始就结束了。</p>
<hr>
<h1 id="番外"><a href="#番外" class="headerlink" title="番外"></a>番外</h1><p>我理想中的爱情是日久生情的那种，而不是一见钟情。</p>
<blockquote>
<p>话说回来，9012年了竟然还有人相信一见钟情，还有人愿意等命中之人，真是……幸福，没有危机感，也不着急，哪怕是自己也知道希望渺茫。</p>
<p>也许是我什么都不懂吧。</p>
</blockquote>
<p>遗憾的是，命运并没有给我安排日久生情的机会，不特意去创造机会的话，恐怕以后也不会安排。所以也就就像，自己的幸福要靠自己去努力争取，的感觉吧。</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title>闲聊</title>
    <url>/2019/09/08/%E9%97%B2%E8%81%8A/</url>
    <content><![CDATA[<p>点点点</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title>测试用</title>
    <url>/2019/09/07/%E6%94%B9%E5%90%8D%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<p> 发新文测试</p>
]]></content>
  </entry>
  <entry>
    <title>wei,zaima</title>
    <url>/2019/08/21/hello-world/</url>
    <content><![CDATA[<p>buzai, cmn</p>
<h1 id="还行"><a href="#还行" class="headerlink" title="还行"></a>还行</h1><p>我佛了</p>
<h1 id="真的吗"><a href="#真的吗" class="headerlink" title="真的吗"></a>真的吗</h1><p>神秘。</p>
<h1 id="激霸"><a href="#激霸" class="headerlink" title="激霸"></a>激霸</h1><p>看到这的建议请客</p>
]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
</search>
